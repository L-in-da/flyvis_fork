
<!doctype html>
<html lang="en" class="no-js">
  <head>

      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">



        <link rel="canonical" href="https://flyvis.github.io/examples/02_flyvision_optic_flow_task/">


        <link rel="prev" href="../01_flyvision_connectome/">


        <link rel="next" href="../03_flyvision_flash_responses/">


      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.38">



        <title>Train the network - flyvis docs</title>



      <link rel="stylesheet" href="../../assets/stylesheets/main.8c3ca2c6.min.css">


        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">












        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>



      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">

      <link rel="stylesheet" href="../../style.css">

    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>






  </head>









    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">


    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">


        <a href="#optic-flow-task" class="md-skip">
          Skip to content
        </a>

    </div>
    <div data-md-component="announce">

    </div>







<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="flyvis docs" class="md-header__button md-logo" aria-label="flyvis docs" data-md-component="logo">

  <img src="../../images/flyvis_logo_light@150 ppi.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">

      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            flyvis docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">

              Train the network

          </span>
        </div>
      </div>
    </div>


        <form class="md-header__option" data-md-component="palette">




    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">

      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>





    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">

      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>





    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">

      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>


</form>



      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>



      <label class="md-header__button md-icon" for="__search">

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">

        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">

          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>

    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>


  </nav>

</header>


    <div class="md-container" data-md-component="container">






      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">



              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="flyvis docs" class="md-nav__button md-logo" aria-label="flyvis docs" data-md-component="logo">

  <img src="../../images/flyvis_logo_light@150 ppi.webp" alt="logo">

    </a>
    flyvis docs
  </label>

  <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">


  <span class="md-ellipsis">
    Home
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../install/" class="md-nav__link">


  <span class="md-ellipsis">
    Installation
  </span>


      </a>
    </li>















    <li class="md-nav__item md-nav__item--active md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>


          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">


  <span class="md-ellipsis">
    Tutorials
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../01_flyvision_connectome/" class="md-nav__link">


  <span class="md-ellipsis">
    Explore the connectome
  </span>


      </a>
    </li>












    <li class="md-nav__item md-nav__item--active">

      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">





      <a href="./" class="md-nav__link md-nav__link--active">


  <span class="md-ellipsis">
    Train the network
  </span>


      </a>

    </li>










    <li class="md-nav__item">
      <a href="../03_flyvision_flash_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Flash responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../04_flyvision_moving_edge_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Moving edge responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../05_flyvision_umap_and_clustering_models/" class="md-nav__link">


  <span class="md-ellipsis">
    Ensemble clustering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../06_flyvision_maximally_excitatory_stimuli/" class="md-nav__link">


  <span class="md-ellipsis">
    Maximally excitatory stimuli
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../07_flyvision_providing_custom_stimuli/" class="md-nav__link">


  <span class="md-ellipsis">
    Custom stimuli
  </span>


      </a>
    </li>




          </ul>
        </nav>

    </li>













    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >


          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">


  <span class="md-ellipsis">
    Main results
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Main results
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../figure_01_fly_visual_system/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 1
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_02_simple_stimuli_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 2
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_03_naturalistic_stimuli_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 3
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_04_mechanisms/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 4
  </span>


      </a>
    </li>




          </ul>
        </nav>

    </li>













    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >


          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">


  <span class="md-ellipsis">
    API Reference
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../../reference/connectome/" class="md-nav__link">


  <span class="md-ellipsis">
    Connectome
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/network/" class="md-nav__link">


  <span class="md-ellipsis">
    Network
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/sintel/" class="md-nav__link">


  <span class="md-ellipsis">
    Sintel
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/tasks/" class="md-nav__link">


  <span class="md-ellipsis">
    Task
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/solver/" class="md-nav__link">


  <span class="md-ellipsis">
    Training
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/flash_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Flash Responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/moving_stimulus_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Moving Stimulus Responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/ensemble_clustering/" class="md-nav__link">


  <span class="md-ellipsis">
    Ensemble Clustering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/optimal_stimuli/" class="md-nav__link">


  <span class="md-ellipsis">
    Optimal Stimuli
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/scripts/" class="md-nav__link">


  <span class="md-ellipsis">
    Scripts
  </span>


      </a>
    </li>














    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_11" >


          <label class="md-nav__link" for="__nav_5_11" id="__nav_5_11_label" tabindex="0">


  <span class="md-ellipsis">
    Miscelleanous
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_11">
            <span class="md-nav__icon md-icon"></span>
            Miscelleanous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../../reference/rendering/" class="md-nav__link">


  <span class="md-ellipsis">
    Rendering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/augmentation/" class="md-nav__link">


  <span class="md-ellipsis">
    Augmentation
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/datasets/" class="md-nav__link">


  <span class="md-ellipsis">
    Datasets
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/utils/" class="md-nav__link">


  <span class="md-ellipsis">
    Utils
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/visualizations/" class="md-nav__link">


  <span class="md-ellipsis">
    Visualizations
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/animations/" class="md-nav__link">


  <span class="md-ellipsis">
    Animations
  </span>


      </a>
    </li>




          </ul>
        </nav>

    </li>




          </ul>
        </nav>

    </li>









    <li class="md-nav__item">
      <a href="../../contribute/" class="md-nav__link">


  <span class="md-ellipsis">
    Contributing
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">


  <span class="md-ellipsis">
    FAQ
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../acknowledgements/" class="md-nav__link">


  <span class="md-ellipsis">
    Acknowledgements
  </span>


      </a>
    </li>



  </ul>
</nav>
                  </div>
                </div>
              </div>



              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">






</nav>
                  </div>
                </div>
              </div>



            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">




<pre><code>%load_ext autoreload
%autoreload 2
</code></pre>
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
</code></pre>
<h1 id="optic-flow-task">Optic flow task</h1>
<p>This notebook illustrates the optic flow task and how to use it with our pretrained fly visual system model and decoder.</p>
<p><strong>Select GPU runtime</strong></p>
<p>To run the notebook on a GPU select Menu -&gt; Runtime -&gt; Change runtime type -&gt; GPU.</p>
<pre><code># @markdown **Check access to GPU**

try:
    import google.colab

    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    import torch

    try:
        cuda_name = torch.cuda.get_device_name()
        print(f&quot;Name of the assigned GPU / CUDA device: {cuda_name}&quot;)
    except RuntimeError:
        import warnings

        warnings.warn(
            &quot;You have not selected Runtime Type: 'GPU' or Google could not assign you one. Please revisit the settings as described above or proceed on CPU (slow).&quot;
        )
</code></pre>
<p><strong>Install Flyvis</strong></p>
<p>The notebook requires installing our package <code>flyvis</code>. You may need to restart your session after running the code block below with Menu -&gt; Runtime -&gt; Restart session. Then, imports from <code>flyvis</code> should succeed without issue.</p>
<pre><code>if IN_COLAB:
    # @markdown **Install Flyvis**
    %%capture
    !git clone https://github.com/flyvis/flyvis-dev.git
    %cd /content/flyvis-dev
    !pip install -e .
</code></pre>
<pre><code># basic imports
import matplotlib.pyplot as plt
import numpy as np
import torch

plt.rcParams['figure.dpi'] = 200
</code></pre>
<h1 id="the-sintel-dataset">The Sintel dataset</h1>
<p>We use the Sintel dataset to train out models as described in the paper. More infos about the Sintel dataset can be found on the official Sintel website: http://sintel.is.tue.mpg.de/.</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
from flyvision.datasets.sintel import MultiTaskSintel
from flyvision.analysis.animations.sintel import SintelSample

%load_ext autoreload
%autoreload 2
</code></pre>
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
</code></pre>
<p>The class <code>MultiTaskSintel</code> loads, preprocesses, renders, and augments the sintel data. It adheres to the pytorch dataset primitive. It provides the interface to the input data and the output data for the flyvision networks. Note: the fly-eye rendering we use here, we introduce in the notebook on creating custom stimuli already.</p>
<p>This is the full setting:</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    # Because the fly eye rendering is square
    # and sintel is wide, we can crop sintel
    # in width and render three sequences from one.
    # This allows us to statically augment our dataset
    # a bit already before we proceed with the random augmentations.
    # We end up with 3 * 23 sequences.
    vertical_splits=3,
    n_frames=19,
    center_crop_fraction=0.7,
    dt=1 / 50,
    augment=True,
    # From sequences with more than n_frames, we randomly sample the start frame.
    random_temporal_crop=True,
    all_frames=False,
    # We resample movie frames to the effective framerate given by 1/dt
    resampling=True,
    # We interpolate the flow arrows to 1/dt.
    interpolate=True,
    # We flip with equal probability (using one flip-axis).
    p_flip=0.5,
    # We rotate with equal probability (using five fold rotation symmetry of the hex-grid).
    p_rot=5 / 6,
    # We randomly adjust contrast and brightness.
    contrast_std=0.2,
    brightness_std=0.1,
    # We add random white noise pixelweise.
    gaussian_white_noise=0.08,
    gamma_std=None,
    _init_cache=True,
    unittest=False,
    flip_axes=[0, 1],
    task_weights=None,
)
</code></pre>
<pre><code># The `dataset.arg_df` tracks the sequence index, identity etc.
dataset.arg_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>original_index</th>
      <th>name</th>
      <th>original_n_frames</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_02</td>
      <td>50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1</td>
      <td>sequence_01_alley_2_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>1</td>
      <td>sequence_01_alley_2_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>64</th>
      <td>64</td>
      <td>21</td>
      <td>sequence_21_temple_2_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>65</th>
      <td>65</td>
      <td>21</td>
      <td>sequence_21_temple_2_split_02</td>
      <td>50</td>
    </tr>
    <tr>
      <th>66</th>
      <td>66</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>67</th>
      <td>67</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>68</th>
      <td>68</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_02</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
<p>69 rows × 4 columns</p>
</div>

<h2 id="single-sample">Single sample</h2>
<p>First, let's chunk this into smaller digestable pieces.</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    dt=1 / 24,
    augment=False,
)
</code></pre>
<p>The first sample. For the target, the pixel-accurate motion vectors, the color indicates the direction of motion of the respective input pixel. The saturation indicates the magnitude of motion.</p>
<pre><code>lum = dataset[0][&quot;lum&quot;]
flow = dataset[0][&quot;flow&quot;]

animation = SintelSample(lum[None], flow[None])
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_17_0.png" /></p>
<p>Sintel has more groundtruth annotations. We support depth and flow because we know with some confidence that these are relevant for the fly.</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;depth&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    dt=1 / 24,
    augment=False,
)
</code></pre>
<pre><code>lum1 = dataset[0][&quot;lum&quot;]
depth1 = dataset[0][&quot;depth&quot;]

animation = SintelSample(lum1[None], depth1[None])
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_20_0.png" /></p>
<h1 id="augmenting-the-dataset-step-by-step">Augmenting the dataset step-by-step</h1>
<p>We apply rich augmentations to the dataset of naturalistic sequences because the dataset is otherwise relatively small. This might lead to overfitting to, e.g., predicting motion mostly into well-represented directons or of objects of specific contrasts etc. Using rich augmentations, we 'ask' the network to generalize better and invariantly compute motion regardless of direction, contrast, brightness, pixel noise, temporal appearance etc.</p>
<h2 id="vertical-splits">Vertical splits</h2>
<p>First, we split each sequence into three sequences vertically to leverage a wider extent of the video than if we would only render the center. We precompute these renderings.</p>
<pre><code>from flyvision.analysis.visualization.plots import quick_hex_scatter
</code></pre>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=3,
    dt=1 / 24,
    augment=False,
)
</code></pre>
<p>Sintel has 23 movie sequences originally.</p>
<pre><code>len(np.unique(dataset.arg_df.original_index))
</code></pre>
<pre><code>23
</code></pre>
<p>Each original sequence is 436 pixel in height times 1024 pixel in width in cartesian coordinates.</p>
<pre><code>sequence = dataset.cartesian_sequence(0, vertical_splits=1, center_crop_fraction=1.0)
print(sequence.shape)
</code></pre>
<pre><code>(1, 49, 436, 1024)
</code></pre>
<p>With the vertical crops, we end up with 3 * 23 sequences. The <code>dataset.arg_df</code> tracks the sequence index, identity etc.</p>
<pre><code>dataset.arg_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>original_index</th>
      <th>name</th>
      <th>original_n_frames</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0</td>
      <td>sequence_00_alley_1_split_02</td>
      <td>50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1</td>
      <td>sequence_01_alley_2_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>1</td>
      <td>sequence_01_alley_2_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>64</th>
      <td>64</td>
      <td>21</td>
      <td>sequence_21_temple_2_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>65</th>
      <td>65</td>
      <td>21</td>
      <td>sequence_21_temple_2_split_02</td>
      <td>50</td>
    </tr>
    <tr>
      <th>66</th>
      <td>66</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_00</td>
      <td>50</td>
    </tr>
    <tr>
      <th>67</th>
      <td>67</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_01</td>
      <td>50</td>
    </tr>
    <tr>
      <th>68</th>
      <td>68</td>
      <td>22</td>
      <td>sequence_22_temple_3_split_02</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
<p>69 rows × 4 columns</p>
</div>

<pre><code>_ = plt.imshow(sequence[0, 0], cmap=plt.cm.binary_r)
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_33_0.png" /></p>
<pre><code>fig, axes = plt.subplots(1, 3)
_ = quick_hex_scatter(dataset[0]['lum'][0].flatten(), fig=fig, ax=axes[0], cbar=False)
_ = quick_hex_scatter(dataset[1]['lum'][0].flatten(), fig=fig, ax=axes[1], cbar=False)
_ = quick_hex_scatter(dataset[2]['lum'][0].flatten(), fig=fig, ax=axes[2], cbar=False)
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_34_0.png" /></p>
<h2 id="random-temporal-crops">Random temporal crops</h2>
<p>We train on 19 frames ~ 792ms movie. Most sequences have 49 frames. To use the whole temporal content, we stochastically sample start and end frame ~ ((1, 19), (2, 20), ..., (31, 49)).</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=3,
    n_frames=19,
    dt=1 / 24,
    augment=True,
    random_temporal_crop=True,
    all_frames=False,
    resampling=False,
    interpolate=False,
    p_flip=0,
    p_rot=0,
    contrast_std=None,
    brightness_std=None,
    gaussian_white_noise=None,
)
</code></pre>
<pre><code># These two samples from the same original sequence should have stochastically different start and end frames.
lum1 = dataset[0]['lum']
lum2 = dataset[0]['lum']
</code></pre>
<pre><code>animation = SintelSample(lum1[None], lum2[None], title2=&quot;input&quot;)
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_39_0.png" /></p>
<h2 id="flips-and-rotations">Flips and rotations</h2>
<p>Next, we flip stochastically across 2 axes and or rotate a random number of times around the center. We implement this to be fast to do so at runtime.</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=3,
    n_frames=19,
    dt=1 / 24,
    augment=True,
    random_temporal_crop=False,
    all_frames=False,
    resampling=False,
    interpolate=False,
    p_flip=1 / 2,
    p_rot=5 / 6,
    contrast_std=None,
    brightness_std=None,
    gaussian_white_noise=None,
)
</code></pre>
<pre><code># These two samples from the same original sequence should have stochastically different orientation.
lum1 = dataset[0]['lum']
lum2 = dataset[0]['lum']
</code></pre>
<pre><code>animation = SintelSample(lum1[None], lum2[None], title2=&quot;input&quot;)
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_44_0.png" /></p>
<p>Flow vectors need to be flipped and rotated accordingly.</p>
<pre><code># These two samples from the same original sequence should have stochastically different orientation.
data = dataset[0]
lum1 = data['lum']
flow1 = data['flow']
</code></pre>
<pre><code>animation = SintelSample(lum1[None], flow1[None])
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_47_0.png" /></p>
<h2 id="further-augmentations">Further augmentations</h2>
<p>Besides that, we also augment the input with random contrasts and brightnesses and random gaussian pixel noise, while the motion stays the same. This pretends that the same motion takes place under different illumination conditions and signal to noise ratios.  </p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=3,
    n_frames=19,
    dt=1 / 24,
    augment=True,
    random_temporal_crop=False,
    all_frames=False,
    resampling=False,
    interpolate=False,
    p_flip=0,
    p_rot=0,
    contrast_std=0.2,
    brightness_std=0.1,
    gaussian_white_noise=0.08,
)
</code></pre>
<pre><code># These two samples from the same original sequence have
# stochastically different contrast, brightness and pixel-wise noise.
lum1 = dataset[0]['lum']
lum2 = dataset[0]['lum']
</code></pre>
<pre><code>animation = SintelSample(lum1[None], lum2[None], title2=&quot;input&quot;)
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_52_0.png" /></p>
<h1 id="framerate-of-the-dataset-and-integration-time-step">Framerate of the dataset and integration time step</h1>
<p>The Sintel dataset is originally rendered at 24 frames per second, i.e., one frame every 42ms. The fruit fly neurons are able to respond to temporal differences as fast as 5-20ms. Therefore, we resample every frame multiple times to pretend that the movie was originally sampled at such a faster framerate. For the motion fields, we interpolate flow vectors in time instead of resampling them, which hopefully gives a better learning signal to the network. We have to trade-off speed of the numerical integration and memory consumption during optimization with the simulation accuracy by choosing time steps between 5-20ms. We chose to train networks at the upper bount of 20ms and evaluate them more accurately at 5-10ms.</p>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=3,
    n_frames=19,
    dt=1 / 50,
    augment=False,
    resampling=True,
    interpolate=True,
)
</code></pre>
<pre><code># Now, every input frame appears twice and target frames are interpolated.
data = dataset[0]
lum1 = data['lum']
flow1 = data['flow']
</code></pre>
<pre><code>animation = SintelSample(lum1[None], flow1[None])
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_57_0.png" /></p>
<h1 id="computing-responses-to-the-sintel-data">Computing responses to the Sintel data</h1>
<p>Before we get to training a network, we look at a few responses to these type of sequences of individual neurons.</p>
<pre><code>from flyvision.network import NetworkView, Network
from flyvision.utils.activity_utils import LayerActivity

from flyvision.datasets.sintel import MultiTaskSintel
</code></pre>
<pre><code># new network instance
network = Network()

# Alternative: uncomment to use a pretrained network
# network_view = NetworkView(results_dir / &quot;flow/0000/000&quot;)
# network = network_view.init_network(network)
</code></pre>
<pre><code>[2024-10-04 14:36:03] network:253 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
</code></pre>
<pre><code>layer_activity = LayerActivity(None, network.connectome, keepref=True)
</code></pre>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    n_frames=19,
    dt=1 / 50,
    augment=False,
    resampling=True,
    interpolate=True,
)
</code></pre>
<pre><code>stationary_state = network.fade_in_state(1.0, dataset.dt, dataset[0][&quot;lum&quot;][[0]])
responses = network.simulate(
    dataset[0][&quot;lum&quot;][None], dataset.dt, initial_state=stationary_state
).cpu()
</code></pre>
<pre><code>plt.figure(figsize=[3, 2])
layer_activity.update(responses)
r = layer_activity.central.T4c.squeeze().numpy()
time = np.arange(0, r.shape[0], 1) * dataset.dt
plt.plot(time, r)
plt.xlabel(&quot;time in s&quot;)
plt.ylabel(&quot;voltage (a.u.)&quot;)
plt.title(&quot;response of central T4c cell&quot;)
</code></pre>
<pre><code>Text(0.5, 1.0, 'response of central T4c cell')
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_65_1.png" /></p>
<h1 id="decoding-the-task-from-neural-activity">Decoding the task from neural activity</h1>
<p>We need to predict the pixel-accurate flow field that Sintel gives us. For that we decode the voltages of a bunch of cell types. The decoder and the network are trained end-to-end. Here an example of a forward pass through the whole pipeline in code.</p>
<pre><code>from flyvision.datasets.sintel import MultiTaskSintel
from flyvision.task.decoder import DecoderGAVP
</code></pre>
<pre><code>network = Network()
</code></pre>
<pre><code>[2024-10-04 14:37:42] network:253 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
</code></pre>
<pre><code>decoder = DecoderGAVP(network.connectome, shape=[8, 2], kernel_size=5)
</code></pre>
<pre><code>[2024-10-04 14:37:46] decoder:215 Initialized decoder with NumberOfParams(free=7427, fixed=0) parameters.
[2024-10-04 14:37:46] decoder:216 DecoderGAVP(
  (base): Sequential(
    (0): Conv2dHexSpace(34, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Softplus(beta=1, threshold=20)
    (3): Dropout(p=0.5, inplace=False)
  )
  (decoder): Sequential(
    (0): Conv2dHexSpace(8, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  )
  (head): Sequential()
)
</code></pre>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    all_frames=True,
    dt=1 / 50,
    augment=False,
    resampling=True,
    interpolate=True,
)
</code></pre>
<pre><code>data = dataset[0]
lum = data[&quot;lum&quot;]
flow = data[&quot;flow&quot;]

stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])
responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)
</code></pre>
<pre><code>y_pred = decoder(responses)
</code></pre>
<p>We predict motion with an untrained decoder from an untrained network with randomly initialized parameters.
We do not expect this to work.</p>
<pre><code>animation = SintelSample(lum[None], flow[None], prediction=y_pred.detach().cpu())
animation.animate_in_notebook(frames=np.arange(lum.shape[0])[::10])
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_74_0.png" /></p>
<pre><code>((y_pred - flow) ** 2).sqrt().mean()
</code></pre>
<pre><code>tensor(0.9850, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h1 id="training-network-and-decoder-on-a-single-batch">Training network and decoder on a single batch</h1>
<p>We now train the network on a single batch to validate that the pipeline works. We do not expect these networks to generalize their function.</p>
<pre><code>from tqdm.notebook import tqdm
from torch.optim import Adam
from torch.utils.data import DataLoader

from flyvision.network import Network
from flyvision.task.decoder import DecoderGAVP

from flyvision.datasets.sintel import MultiTaskSintel
from flyvision.task.objectives import l2norm, epe
</code></pre>
<pre><code>network = Network()
</code></pre>
<pre><code>[2024-10-04 14:38:50] network:253 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
</code></pre>
<pre><code>decoder = DecoderGAVP(network.connectome, shape=[8, 2], kernel_size=5)
</code></pre>
<pre><code>[2024-10-04 14:38:54] decoder:215 Initialized decoder with NumberOfParams(free=7427, fixed=0) parameters.
[2024-10-04 14:38:54] decoder:216 DecoderGAVP(
  (base): Sequential(
    (0): Conv2dHexSpace(34, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Softplus(beta=1, threshold=20)
    (3): Dropout(p=0.5, inplace=False)
  )
  (decoder): Sequential(
    (0): Conv2dHexSpace(8, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  )
  (head): Sequential()
)
</code></pre>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    n_frames=19,
    dt=1 / 50,
    augment=False,
    resampling=True,
    interpolate=True,
)
</code></pre>
<pre><code>t_pre = 0.5
dt = 1 / 50
batch_size = 4
train_loader = DataLoader(dataset, batch_size=batch_size)
</code></pre>
<pre><code>optimizer = Adam((*network.parameters(), *decoder.parameters()), lr=1e-5)
</code></pre>
<pre><code>batch = next(iter(train_loader))
</code></pre>
<pre><code>loss_fn = epe
</code></pre>
<pre><code>epochs = 1000

errors = []

initial_state = network.steady_state(t_pre, dt, batch_size)

for e in tqdm(range(epochs)):
    lum = batch[&quot;lum&quot;]
    flow = batch[&quot;flow&quot;]

    optimizer.zero_grad()
    network.stimulus.zero()
    network.stimulus.add_input(lum)

    activity = network(network.stimulus(), dt=1 / 50, state=initial_state)
    y_pred = decoder(activity)

    batch_error = loss_fn(y_pred, flow)
    errors.append(batch_error.cpu().item())
    batch_error.backward()
    optimizer.step()

    if e % 10 == 0:
        print(f&quot;Epoch {e}: {batch_error.item()}&quot;)
</code></pre>
<pre><code>  0%|          | 0/1000 [00:00&lt;?, ?it/s]


Epoch 0: 10.422589302062988
Epoch 10: 10.42231559753418
Epoch 20: 10.420804023742676
Epoch 30: 10.420122146606445
Epoch 40: 10.419205665588379
Epoch 50: 10.41639518737793
Epoch 60: 10.416065216064453
Epoch 70: 10.414222717285156
Epoch 80: 10.413128852844238
Epoch 90: 10.410683631896973
Epoch 100: 10.410036087036133
Epoch 110: 10.408438682556152
Epoch 120: 10.408326148986816
Epoch 130: 10.40695858001709
Epoch 140: 10.405557632446289
Epoch 150: 10.400578498840332
Epoch 160: 10.40015983581543
Epoch 170: 10.398872375488281
Epoch 180: 10.396742820739746
Epoch 190: 10.392280578613281
Epoch 200: 10.393349647521973
Epoch 210: 10.390533447265625
Epoch 220: 10.388550758361816
Epoch 230: 10.385358810424805
Epoch 240: 10.382622718811035
Epoch 250: 10.381763458251953
Epoch 260: 10.377959251403809
Epoch 270: 10.373452186584473
Epoch 280: 10.369023323059082
Epoch 290: 10.366665840148926
Epoch 300: 10.365363121032715
Epoch 310: 10.358964920043945
Epoch 320: 10.35590648651123
Epoch 330: 10.35021686553955
Epoch 340: 10.34598445892334
Epoch 350: 10.34181022644043
Epoch 360: 10.33405876159668
Epoch 370: 10.323880195617676
Epoch 380: 10.319880485534668
Epoch 390: 10.316190719604492
Epoch 400: 10.311019897460938
Epoch 410: 10.305534362792969
Epoch 420: 10.30172061920166
Epoch 430: 10.298951148986816
Epoch 440: 10.292269706726074
Epoch 450: 10.2912015914917
Epoch 460: 10.282553672790527
Epoch 470: 10.276850700378418
Epoch 480: 10.271702766418457
Epoch 490: 10.266114234924316
Epoch 500: 10.260465621948242
Epoch 510: 10.254273414611816
Epoch 520: 10.25048542022705
Epoch 530: 10.243037223815918
Epoch 540: 10.243624687194824
Epoch 550: 10.234699249267578
Epoch 560: 10.231546401977539
Epoch 570: 10.224716186523438
Epoch 580: 10.221471786499023
Epoch 590: 10.218791007995605
Epoch 600: 10.213826179504395
Epoch 610: 10.209129333496094
Epoch 620: 10.20657730102539
Epoch 630: 10.202775955200195
Epoch 640: 10.199694633483887
Epoch 650: 10.197911262512207
Epoch 660: 10.195591926574707
Epoch 670: 10.186054229736328
Epoch 680: 10.186713218688965
Epoch 690: 10.183159828186035
Epoch 700: 10.181544303894043
Epoch 710: 10.178598403930664
Epoch 720: 10.170660972595215
Epoch 730: 10.169751167297363
Epoch 740: 10.167207717895508
Epoch 750: 10.16340160369873
Epoch 760: 10.166637420654297
Epoch 770: 10.158493041992188
Epoch 780: 10.155372619628906
Epoch 790: 10.154718399047852
Epoch 800: 10.150259971618652
Epoch 810: 10.146480560302734
Epoch 820: 10.145279884338379
Epoch 830: 10.141718864440918
Epoch 840: 10.143719673156738
Epoch 850: 10.135321617126465
Epoch 860: 10.134828567504883
Epoch 870: 10.129217147827148
Epoch 880: 10.1298828125
Epoch 890: 10.130807876586914
Epoch 900: 10.124497413635254
Epoch 910: 10.12447738647461
Epoch 920: 10.120392799377441
Epoch 930: 10.114103317260742
Epoch 940: 10.114625930786133
Epoch 950: 10.111650466918945
Epoch 960: 10.110936164855957
Epoch 970: 10.106573104858398
Epoch 980: 10.10794448852539
Epoch 990: 10.11015510559082
</code></pre>
<pre><code>plt.plot(errors)
</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f7f5467f580&gt;]
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_86_1.png" /></p>
<p>We expect that the prediction from this overfitted network on the sample it was trained on is ok.</p>
<pre><code>data = dataset[0]
lum = data[&quot;lum&quot;]
flow = data[&quot;flow&quot;]

stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])
responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)
</code></pre>
<pre><code>y_pred = decoder(responses)
</code></pre>
<pre><code>animation = SintelSample(lum[None], flow[None], prediction=y_pred.detach().cpu())
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_90_0.png" /></p>
<pre><code>((y_pred - flow) ** 2).sqrt().mean()
</code></pre>
<pre><code>tensor(0.8250, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h1 id="evaluating-trained-networks">Evaluating trained networks</h1>
<pre><code>from flyvision import results_dir
from flyvision.network import NetworkView
from flyvision.utils.activity_utils import LayerActivity

from flyvision.datasets.sintel import MultiTaskSintel
from flyvision.task.decoder import DecoderGAVP
</code></pre>
<pre><code># we load the best task-performing model from the presorted ensemble
network_view = NetworkView(results_dir / &quot;flow/0000/000&quot;)
</code></pre>
<pre><code>[2024-09-23 15:54:46] network:1005 Initialized network view at /groups/turaga/home/lappalainenj/FlyVis/private/flyvision/data/results/flow/0000/000.
</code></pre>
<pre><code>network = network_view.init_network()
</code></pre>
<pre><code>[2024-09-23 15:54:55] network:252 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
[2024-09-23 15:54:55] chkpt_utils:72 Recovered network state.
</code></pre>
<pre><code>decoder = network_view.init_decoder()[&quot;flow&quot;]
</code></pre>
<pre><code>[2024-09-23 15:54:56] chkpt_utils:72 Recovered network state.
[2024-09-23 15:55:00] decoder:213 Initialized decoder with NumberOfParams(free=7427, fixed=0) parameters.
[2024-09-23 15:55:00] decoder:214 DecoderGAVP(
  (base): Sequential(
    (0): Conv2dHexSpace(34, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Softplus(beta=1, threshold=20)
    (3): Dropout(p=0.5, inplace=False)
  )
  (decoder): Sequential(
    (0): Conv2dHexSpace(8, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  )
  (head): Sequential()
)
[2024-09-23 15:55:00] chkpt_utils:91 Recovered flow decoder state.
</code></pre>
<pre><code>dataset = MultiTaskSintel(
    tasks=[&quot;flow&quot;],
    boxfilter=dict(extent=15, kernel_size=13),
    vertical_splits=1,
    all_frames=False,
    n_frames=19,
    dt=1 / 50,
    augment=False,
    resampling=True,
    interpolate=True,
)
</code></pre>
<pre><code>data = [dataset[i] for i in range(4)]
lum = torch.stack([d[&quot;lum&quot;] for d in data])
flow = torch.stack([d[&quot;flow&quot;] for d in data])

stationary_state = network.fade_in_state(1.0, dataset.dt, lum[:, 0])
responses = network.simulate(lum, dataset.dt, initial_state=stationary_state)
</code></pre>
<pre><code>y_pred = decoder(responses)
</code></pre>
<p>We expect this network to generalize across sequences. This network sees motion into all directions.</p>
<pre><code>animation = SintelSample(lum, flow, prediction=y_pred.detach().cpu())
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_101_0.png" /></p>
<p>We expect the accuracy is not as good as the overfitted example because this network generalized across the whole-dataset.</p>
<pre><code>((y_pred - flow) ** 2).sqrt().mean()
</code></pre>
<pre><code>tensor(6.4063, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)
</code></pre>
<h1 id="evaluating-ensembles">Evaluating ensembles</h1>
<p>Last, we evaluated the task error of the 50 trained networks on a held out set of sequences. We evaluated the task error across all checkpoints during training and show the minimal one in the histrogram below. This checkpoint we analyse with respect to it's tuning predictions as shown in the next notebooks.</p>
<pre><code>from flyvision import EnsembleView
</code></pre>
<pre><code>ensemble = EnsembleView(results_dir / &quot;flow/0000&quot;)
</code></pre>
<pre><code>Loading ensemble:   0%|          | 0/50 [00:00&lt;?, ?it/s]


[2024-09-23 15:57:24] ensemble:138 Loaded 50 networks.
</code></pre>
<pre><code>ensemble.task_error_histogram()
</code></pre>
<pre><code>(&lt;Figure size 300x300 with 1 Axes&gt;,
 &lt;Axes: xlabel='task error', ylabel='number models'&gt;)
</code></pre>
<p><img alt="png" src="../02_flyvision_optic_flow_task_files/02_flyvision_optic_flow_task_108_1.png" /></p>













              </article>
            </div>


<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>

      </main>

        <footer class="md-footer">

  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">


    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>

</div>

    </div>
  </div>
</footer>

    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>


    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>


      <script src="../../assets/javascripts/bundle.d6f25eb3.min.js"></script>

        <script src="../../javascripts/switch-logo.js"></script>


  </body>
</html>
