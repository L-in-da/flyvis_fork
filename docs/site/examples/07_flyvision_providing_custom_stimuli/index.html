
<!doctype html>
<html lang="en" class="no-js">
  <head>

      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">



        <link rel="canonical" href="https://flyvis.github.io/examples/07_flyvision_providing_custom_stimuli/">


        <link rel="prev" href="../06_flyvision_maximally_excitatory_stimuli/">


        <link rel="next" href="../figure_01_fly_visual_system/">


      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.38">



        <title>Custom stimuli - flyvis docs</title>



      <link rel="stylesheet" href="../../assets/stylesheets/main.8c3ca2c6.min.css">


        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">












        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>



      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">

      <link rel="stylesheet" href="../../style.css">

    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>






  </head>









    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">


    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">


        <a href="#providing-custom-stimuli" class="md-skip">
          Skip to content
        </a>

    </div>
    <div data-md-component="announce">

    </div>







<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="flyvis docs" class="md-header__button md-logo" aria-label="flyvis docs" data-md-component="logo">

  <img src="../../images/flyvis_logo_light@150 ppi.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">

      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            flyvis docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">

              Custom stimuli

          </span>
        </div>
      </div>
    </div>


        <form class="md-header__option" data-md-component="palette">




    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">

      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>





    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">

      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>





    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">

      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>


</form>



      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>



      <label class="md-header__button md-icon" for="__search">

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>

        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">

        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">

          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>

    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>


  </nav>

</header>


    <div class="md-container" data-md-component="container">






      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">



              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="flyvis docs" class="md-nav__button md-logo" aria-label="flyvis docs" data-md-component="logo">

  <img src="../../images/flyvis_logo_light@150 ppi.webp" alt="logo">

    </a>
    flyvis docs
  </label>

  <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">


  <span class="md-ellipsis">
    Home
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../install/" class="md-nav__link">


  <span class="md-ellipsis">
    Installation
  </span>


      </a>
    </li>















    <li class="md-nav__item md-nav__item--active md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>


          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">


  <span class="md-ellipsis">
    Tutorials
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../01_flyvision_connectome/" class="md-nav__link">


  <span class="md-ellipsis">
    Explore the connectome
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../02_flyvision_optic_flow_task/" class="md-nav__link">


  <span class="md-ellipsis">
    Train the network
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../03_flyvision_flash_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Flash responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../04_flyvision_moving_edge_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Moving edge responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../05_flyvision_umap_and_clustering_models/" class="md-nav__link">


  <span class="md-ellipsis">
    Ensemble clustering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../06_flyvision_maximally_excitatory_stimuli/" class="md-nav__link">


  <span class="md-ellipsis">
    Maximally excitatory stimuli
  </span>


      </a>
    </li>












    <li class="md-nav__item md-nav__item--active">

      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">





        <label class="md-nav__link md-nav__link--active" for="__toc">


  <span class="md-ellipsis">
    Custom stimuli
  </span>


          <span class="md-nav__icon md-icon"></span>
        </label>

      <a href="./" class="md-nav__link md-nav__link--active">


  <span class="md-ellipsis">
    Custom stimuli
  </span>


      </a>



<nav class="md-nav md-nav--secondary" aria-label="Table of contents">






    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>

        <li class="md-nav__item">
  <a href="#example-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Example dataset
    </span>
  </a>

</li>

        <li class="md-nav__item">
  <a href="#boxeye-rendering" class="md-nav__link">
    <span class="md-ellipsis">
      BoxEye rendering
    </span>
  </a>

    <nav class="md-nav" aria-label="BoxEye rendering">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#rendering-cartesian-images-to-hexagonal-lattice" class="md-nav__link">
    <span class="md-ellipsis">
      Rendering cartesian images to hexagonal lattice
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#render-a-single-frame" class="md-nav__link">
    <span class="md-ellipsis">
      Render a single frame
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#render-a-whole-dataset-to-disk" class="md-nav__link">
    <span class="md-ellipsis">
      Render a whole dataset to disk
    </span>
  </a>

</li>

        <li class="md-nav__item">
  <a href="#create-a-sequence-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Create a sequence dataset
    </span>
  </a>

    <nav class="md-nav" aria-label="Create a sequence dataset">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#create-a-custom-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Create a custom dataset
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#compute-model-responses-to-custom-stimuli" class="md-nav__link">
    <span class="md-ellipsis">
      Compute model responses to custom stimuli
    </span>
  </a>

    <nav class="md-nav" aria-label="Compute model responses to custom stimuli">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#select-a-pretrained-network" class="md-nav__link">
    <span class="md-ellipsis">
      Select a pretrained network
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#compute-a-stationary-state" class="md-nav__link">
    <span class="md-ellipsis">
      Compute a stationary state
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#obtain-network-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Obtain network responses
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#visualize-responses-of-specific-cells" class="md-nav__link">
    <span class="md-ellipsis">
      Visualize responses of specific cells
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#compute-responses-over-the-whole-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Compute responses over the whole ensemble
    </span>
  </a>

    <nav class="md-nav" aria-label="Compute responses over the whole ensemble">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#select-the-pretrained-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Select the pretrained ensemble
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#simulate-responses-for-each-network" class="md-nav__link">
    <span class="md-ellipsis">
      Simulate responses for each network
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#visualize-responses-of-specific-cells-across-the-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Visualize responses of specific cells across the ensemble
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

    </ul>

</nav>

    </li>




          </ul>
        </nav>

    </li>













    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >


          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">


  <span class="md-ellipsis">
    Main results
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Main results
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../figure_01_fly_visual_system/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 1
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_02_simple_stimuli_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 2
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_03_naturalistic_stimuli_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 3
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../figure_04_mechanisms/" class="md-nav__link">


  <span class="md-ellipsis">
    Figure 4
  </span>


      </a>
    </li>




          </ul>
        </nav>

    </li>













    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >


          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">


  <span class="md-ellipsis">
    API Reference
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../../reference/connectome/" class="md-nav__link">


  <span class="md-ellipsis">
    Connectome
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/network/" class="md-nav__link">


  <span class="md-ellipsis">
    Network
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/sintel/" class="md-nav__link">


  <span class="md-ellipsis">
    Sintel
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/tasks/" class="md-nav__link">


  <span class="md-ellipsis">
    Task
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/solver/" class="md-nav__link">


  <span class="md-ellipsis">
    Training
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/flash_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Flash Responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/moving_stimulus_responses/" class="md-nav__link">


  <span class="md-ellipsis">
    Moving Stimulus Responses
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/ensemble_clustering/" class="md-nav__link">


  <span class="md-ellipsis">
    Ensemble Clustering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/optimal_stimuli/" class="md-nav__link">


  <span class="md-ellipsis">
    Optimal Stimuli
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/scripts/" class="md-nav__link">


  <span class="md-ellipsis">
    Scripts
  </span>


      </a>
    </li>














    <li class="md-nav__item md-nav__item--nested">



        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_11" >


          <label class="md-nav__link" for="__nav_5_11" id="__nav_5_11_label" tabindex="0">


  <span class="md-ellipsis">
    Miscelleanous
  </span>


            <span class="md-nav__icon md-icon"></span>
          </label>

        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_11">
            <span class="md-nav__icon md-icon"></span>
            Miscelleanous
          </label>
          <ul class="md-nav__list" data-md-scrollfix>







    <li class="md-nav__item">
      <a href="../../reference/rendering/" class="md-nav__link">


  <span class="md-ellipsis">
    Rendering
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/augmentation/" class="md-nav__link">


  <span class="md-ellipsis">
    Augmentation
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/datasets/" class="md-nav__link">


  <span class="md-ellipsis">
    Datasets
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/utils/" class="md-nav__link">


  <span class="md-ellipsis">
    Utils
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/visualizations/" class="md-nav__link">


  <span class="md-ellipsis">
    Visualizations
  </span>


      </a>
    </li>










    <li class="md-nav__item">
      <a href="../../reference/animations/" class="md-nav__link">


  <span class="md-ellipsis">
    Animations
  </span>


      </a>
    </li>




          </ul>
        </nav>

    </li>




          </ul>
        </nav>

    </li>









    <li class="md-nav__item">
      <a href="../../contribute/" class="md-nav__link">


  <span class="md-ellipsis">
    Contributing
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">


  <span class="md-ellipsis">
    FAQ
  </span>


      </a>
    </li>









    <li class="md-nav__item">
      <a href="../../acknowledgements/" class="md-nav__link">


  <span class="md-ellipsis">
    Acknowledgements
  </span>


      </a>
    </li>



  </ul>
</nav>
                  </div>
                </div>
              </div>



              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">






    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>

        <li class="md-nav__item">
  <a href="#example-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Example dataset
    </span>
  </a>

</li>

        <li class="md-nav__item">
  <a href="#boxeye-rendering" class="md-nav__link">
    <span class="md-ellipsis">
      BoxEye rendering
    </span>
  </a>

    <nav class="md-nav" aria-label="BoxEye rendering">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#rendering-cartesian-images-to-hexagonal-lattice" class="md-nav__link">
    <span class="md-ellipsis">
      Rendering cartesian images to hexagonal lattice
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#render-a-single-frame" class="md-nav__link">
    <span class="md-ellipsis">
      Render a single frame
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#render-a-whole-dataset-to-disk" class="md-nav__link">
    <span class="md-ellipsis">
      Render a whole dataset to disk
    </span>
  </a>

</li>

        <li class="md-nav__item">
  <a href="#create-a-sequence-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Create a sequence dataset
    </span>
  </a>

    <nav class="md-nav" aria-label="Create a sequence dataset">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#create-a-custom-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Create a custom dataset
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#compute-model-responses-to-custom-stimuli" class="md-nav__link">
    <span class="md-ellipsis">
      Compute model responses to custom stimuli
    </span>
  </a>

    <nav class="md-nav" aria-label="Compute model responses to custom stimuli">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#select-a-pretrained-network" class="md-nav__link">
    <span class="md-ellipsis">
      Select a pretrained network
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#compute-a-stationary-state" class="md-nav__link">
    <span class="md-ellipsis">
      Compute a stationary state
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#obtain-network-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Obtain network responses
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#visualize-responses-of-specific-cells" class="md-nav__link">
    <span class="md-ellipsis">
      Visualize responses of specific cells
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

        <li class="md-nav__item">
  <a href="#compute-responses-over-the-whole-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Compute responses over the whole ensemble
    </span>
  </a>

    <nav class="md-nav" aria-label="Compute responses over the whole ensemble">
      <ul class="md-nav__list">

          <li class="md-nav__item">
  <a href="#select-the-pretrained-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Select the pretrained ensemble
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#simulate-responses-for-each-network" class="md-nav__link">
    <span class="md-ellipsis">
      Simulate responses for each network
    </span>
  </a>

</li>

          <li class="md-nav__item">
  <a href="#visualize-responses-of-specific-cells-across-the-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Visualize responses of specific cells across the ensemble
    </span>
  </a>

</li>

      </ul>
    </nav>

</li>

    </ul>

</nav>
                  </div>
                </div>
              </div>



            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">




<h1 id="providing-custom-stimuli">Providing custom stimuli</h1>
<p>Follow this notebook to learn how to use our models for generating hypothesis about neural computations with custom stimuli.</p>
<p><strong>Select GPU runtime</strong></p>
<p>To run the notebook on a GPU select Menu -&gt; Runtime -&gt; Change runtime type -&gt; GPU.</p>
<pre><code class="language-python"># @markdown **Check access to GPU**

try:
    import google.colab

    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    import torch

    try:
        cuda_name = torch.cuda.get_device_name()
        print(f&quot;Name of the assigned GPU / CUDA device: {cuda_name}&quot;)
    except RuntimeError:
        import warnings

        warnings.warn(
            &quot;You have not selected Runtime Type: 'GPU' or Google could not assign you one. Please revisit the settings as described above or proceed on CPU (slow).&quot;
        )
</code></pre>
<p><strong>Install Flyvis</strong></p>
<p>The notebook requires installing our package <code>flyvis</code>. You may need to restart your session after running the code block below with Menu -&gt; Runtime -&gt; Restart session. Then, imports from <code>flyvis</code> should succeed without issue.</p>
<pre><code class="language-python">if IN_COLAB:
    # @markdown **Install Flyvis**
    %%capture
    !git clone https://github.com/flyvis/flyvis-dev.git
    %cd /content/flyvis-dev
    !pip install -e .
</code></pre>
<h2 id="example-dataset">Example dataset</h2>
<p>We take the public <a href="https://www.cs.toronto.edu/~nitish/unsupervised_video/">Moving MNIST</a> sequence dataset as an example for a custom stimulus dataset.
Moving MNIST consists of short grey-scale videos of numbers from 1-10 which move in arbitrary directions. The dataset entails 10,000 sequences of 20 frames each. Individual frames are 64x64 pixels in height and width.</p>
<pre><code class="language-python">import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
from flyvision.analysis import animations
</code></pre>
<pre><code class="language-python">sequences = load_moving_mnist()
</code></pre>
<pre><code class="language-python"># the whole dataset has dims (n_sequences, n_frames, height, width)
sequences.shape
</code></pre>
<pre><code>(10000, 20, 64, 64)
</code></pre>
<pre><code class="language-python">animation = animations.Imshow(sequences, cmap=plt.cm.binary_r)
animation.animate_in_notebook(samples=[0, 1, 2])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_9_0.png" /></p>
<p>Alternative: for an alternative dataset that is generated at runtime and does not require a download try <code>random_walk_of_blocks</code>. As a simple drop-in replacement, this requires to replace <code>load_moving_mnist</code> with <code>random_walk_of_blocks</code> across the notebook.</p>
<pre><code class="language-python">from flyvision.utils.dataset_utils import random_walk_of_blocks
</code></pre>
<pre><code class="language-python">sequences = random_walk_of_blocks()
</code></pre>
<pre><code class="language-python">animation = animations.Imshow(sequences, cmap=plt.cm.binary_r)
animation.animate_in_notebook(samples=[0, 1, 2])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_13_0.png" /></p>
<h2 id="boxeye-rendering">BoxEye rendering</h2>
<h5 id="rendering-cartesian-images-to-hexagonal-lattice">Rendering cartesian images to hexagonal lattice</h5>
<p>We translate cartesian frames into receptor activations by placing simulated photoreceptors in a two-dimensional hexagonal array in pixel space (blue dots below), 31 columns across resulting in 721 columns in total, spaced 13 pixels apart. The transduced luminance at each photoreceptor is the greyscale mean value in the 13×13-pixel region surrounding it (black boxes).</p>
<pre><code class="language-python">import flyvision
from flyvision.datasets.rendering import BoxEye
</code></pre>
<pre><code class="language-python">receptors = BoxEye(extent=15, kernel_size=13)
</code></pre>
<pre><code class="language-python">fig = receptors.illustrate()
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_17_0.png" /></p>
<h3 id="render-a-single-frame">Render a single frame</h3>
<p>To illustrate, this is what rendering a single frame looks like.</p>
<pre><code class="language-python">import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
from flyvision.analysis.visualization import plt_utils, plots
</code></pre>
<pre><code class="language-python">sequences = load_moving_mnist()
</code></pre>
<pre><code class="language-python">fig, ax = plt_utils.init_plot(figsize=[1, 1], fontsize=5)
ax = plt_utils.rm_spines(ax)
ax.imshow(sequences[0, 0], cmap=plt.cm.binary_r)
_ = ax.set_title('example frame', fontsize=5)
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_21_0.png" /></p>
<pre><code class="language-python">single_frame = sequences[0, 0]

# the rendering uses pytorch native Conv2d module so it can be executed on GPU and fast
# we first move the frame to GPU
single_frame = torch.tensor(single_frame, device=flyvision.device).float()

# because the inputs to the receptors instance must have four dimensions (samples, frames, height, width),
# we create two empty dimensions for samples and frames
single_frame = single_frame[None, None]
</code></pre>
<pre><code class="language-python"># to render the single frame we simply call the instance
# this automatically rescales the frame to match the receptor layout as illustrated above
# and then places the average pixel value of the 13x13 boxes at the receptor positions
receptors = BoxEye()
rendered = receptors(single_frame)
</code></pre>
<pre><code>/home/lappalainenj@hhmi.org/miniconda3/envs/flyvision/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
</code></pre>
<pre><code class="language-python"># the 721 receptor coordinates are implicitly given in the last dimension
# they correspond to sorted hexagonal coordinates (u-coordinate, v-coordinate, value)
rendered.shape
</code></pre>
<pre><code>torch.Size([1, 1, 1, 721])
</code></pre>
<pre><code class="language-python"># the rendered frame is a slightly blurred version of the example
fig, ax, _ = plots.quick_hex_scatter(
    rendered.squeeze(), vmin=0, vmax=1, cbar_x_offset=0, fontsize=5
)
_ = ax.set_title(&quot;example frame rendered&quot;, fontsize=5)
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_25_0.png" /></p>
<pre><code class="language-python"># Disclaimer: thinking in hex coordinates can be unfamiliar.
# Therefore, we circumvent dealing with them explicitly.
# Still - to understand how the above plot infers the pixel-plane coordinates
# from the implicit hexagonal coordinates, you can inspect the following code.
</code></pre>
<pre><code class="language-python"># # we can explicitly create sorted hex-coordinates from the integer radius of the hexagonal grid
# # for a regular hexagonal lattice, the radius is uniquely determined from the number of hexagons

# radius = flyvision.utils.hex_utils.get_hextent(rendered.shape[-1])

# # here we create integer u, v coordinates, and we stick to the same function and convention
# # everywhere in the code
# u, v = flyvision.utils.hex_utils.get_hex_coords(radius)

# # we transform them to pixel coordinates using our convention
# x, y = flyvision.utils.hex_utils.hex_to_pixel(u, v)

# # and can just scatter them to be back at the photoreceptor layout
# fig, ax = plt_utils.init_plot(figsize=[2, 2], fontsize=5)
# ax.scatter(x, y, s=0.5)
</code></pre>
<h2 id="render-a-whole-dataset-to-disk">Render a whole dataset to disk</h2>
<p>We save rendered sequences to disk to retrieve them faster at runtime.</p>
<p>We will use our library datamate here because it provides a powerful interface for writing and reading arrayfiles.</p>
<pre><code class="language-python">from typing import List
from tqdm import tqdm
import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

from pathlib import Path
from datamate import root, Directory

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
</code></pre>
<pre><code class="language-python"># the Directory class is a smart pointer to a specific directory
# on the filesystem

# directory to store the rendered stimuli
from flyvision import renderings_dir


# root tells where the Directory-tree starts
@root(renderings_dir)
class RenderedData(Directory):
    class Config(dict):
        extent: int  # radius, in number of receptors of the hexagonal array.
        kernel_size: int  # photon collection radius, in pixels.
        subset_idx: List[int]  # if specified, subset of sequences to render

    def __init__(self, config: Config):
        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.
        # this code will be executed automatically once for each unique configuration to store preprocessed
        # data on disk and later simply provide a reference to it.
        sequences = load_moving_mnist()

        # we use the configuration to control the settings under which we render the stimuli
        receptors = BoxEye(extent=config.extent, kernel_size=config.kernel_size)

        # for memory-friendly rendering we can loop over individual sequences
        # and subsets of the dataset
        rendered_sequences = []
        subset_idx = getattr(config, &quot;subset_idx&quot;, []) or list(range(sequences.shape[0]))
        with tqdm(total=len(subset_idx)) as pbar:
            for index in subset_idx:
                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())
                pbar.update()

        # to join individual sequences along their first dimension
        # to obtain (n_sequences, n_frames, 1, receptors.hexals)
        rendered_sequences = np.concatenate(rendered_sequences, axis=0)

        # the __setattr__ method of the Directory class saves sequences to self.path/&quot;sequences.h5&quot;
        # that can be later retrieved using self.sequences[:]
        self.sequences = rendered_sequences
</code></pre>
<pre><code class="language-python"># note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument
moving_mnist_rendered = RenderedData(
    dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3])
)
</code></pre>
<pre><code class="language-python"># this is how we can retrieve the sequences from the disk into memory
rendered_sequences = moving_mnist_rendered.sequences[:]
</code></pre>
<pre><code class="language-python">rendered_sequences.shape
</code></pre>
<pre><code>(4, 20, 1, 721)
</code></pre>
<pre><code class="language-python">animation = animations.HexScatter(rendered_sequences, vmin=0, vmax=1)
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_34_0.png" /></p>
<pre><code class="language-python"># Note, to delete a Directory, e.g. to change the __init__ and reinstantiate,
# run moving_mnist_rendered.rmtree(&quot;y&quot;).
</code></pre>
<h2 id="create-a-sequence-dataset">Create a sequence dataset</h2>
<p>Next we create a Pytorch dataset for loading the sequences.</p>
<pre><code class="language-python">from typing import List
from tqdm import tqdm
import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

from pathlib import Path
from datamate import root, Directory

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
from flyvision.datasets.datasets import SequenceDataset
from flyvision.analysis import animations
</code></pre>
<pre><code class="language-python"># the Directory class is a smart pointer to a specific directory
# on the filesystem

# directory to store the rendered stimuli
from flyvision import renderings_dir


# root tells where the Directory-tree starts
@root(renderings_dir)
class RenderedData(Directory):
    class Config(dict):
        extent: int  # radius, in number of receptors of the hexagonal array.
        kernel_size: int  # photon collection radius, in pixels.
        subset_idx: List[int]  # if specified, subset of sequences to render

    def __init__(self, config: Config):
        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.
        # this code will be executed automatically once for each unique configuration to store preprocessed
        # data on disk and later simply provide a reference to it.
        sequences = load_moving_mnist()

        # we use the configuration to control the settings under which we render the stimuli
        receptors = BoxEye(extent=config.extent, kernel_size=config.kernel_size)

        # for memory-friendly rendering we can loop over individual sequences
        # and subsets of the dataset
        rendered_sequences = []
        subset_idx = getattr(config, &quot;subset_idx&quot;, []) or list(range(sequences.shape[0]))
        with tqdm(total=len(subset_idx)) as pbar:
            for index in subset_idx:
                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())
                pbar.update()

        # to join individual sequences along their first dimension
        # to obtain (n_sequences, n_frames, 1, receptors.hexals)
        rendered_sequences = np.concatenate(rendered_sequences, axis=0)

        # the __setattr__ method of the Directory class saves sequences to self.path/&quot;sequences.h5&quot;
        # that can be later retrieved using self.sequences[:]
        self.sequences = rendered_sequences
</code></pre>
<h5 id="create-a-custom-dataset">Create a custom dataset</h5>
<p>We create a generic interface for custom datasets to make dataloading consistent---this interface can tell the sampler what the framerate, the integration time steps, durations for pre-, and post grey-scale stimulation, and the number of sequences are.</p>
<p>In this case, we inherit a SequenceDataset, that also obeys (and extends) the interface of Pytorch's Dataset.</p>
<pre><code class="language-python">class CustomStimuli(SequenceDataset):
    # implementing the SequenceDataset interface
    dt = 1 / 100
    framerate = 24
    t_pre = 0.5
    t_post = 0.5
    n_sequences = None
    augment = False

    def __init__(self, rendered_data_config: dict):
        self.dir = RenderedData(rendered_data_config)
        self.sequences = torch.tensor(self.dir.sequences[:])
        self.n_sequences = self.sequences.shape[0]

    def get_item(self, key):
        sequence = self.sequences[key]
        # to match the framerate to the integration time dt, we can resample frames
        # from these indices. note, when dt = 1/framerate, this will return the exact sequence
        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])
        return sequence[resample]
</code></pre>
<pre><code class="language-python"># note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument
data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))
</code></pre>
<pre><code class="language-python">data[0].shape
</code></pre>
<pre><code>torch.Size([84, 1, 721])
</code></pre>
<pre><code class="language-python">animation = animations.HexScatter(data[0][None], vmin=0, vmax=1)
animation.animate_in_notebook()
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_43_0.png" /></p>
<h2 id="compute-model-responses-to-custom-stimuli">Compute model responses to custom stimuli</h2>
<p>Now, we can compute model responses across individual models or the whole ensemble to our custom stimulus.</p>
<pre><code class="language-python">from typing import List
from tqdm import tqdm
import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

from pathlib import Path
from datamate import root, Directory

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
from flyvision.datasets.datasets import SequenceDataset
from flyvision.utils.activity_utils import LayerActivity
</code></pre>
<pre><code class="language-python"># the Directory class is a smart pointer to a specific directory
# on the filesystem

# directory to store the rendered stimuli
from flyvision import renderings_dir


# root tells where the Directory-tree starts
@root(renderings_dir)
class RenderedData(Directory):
    class Config(dict):
        extent: int  # radius, in number of receptors of the hexagonal array.
        kernel_size: int  # photon collection radius, in pixels.
        subset_idx: List[int]  # if specified, subset of sequences to render

    def __init__(self, config: Config):
        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.
        # this code will be executed automatically once for each unique configuration to store preprocessed
        # data on disk and later simply provide a reference to it.
        sequences = load_moving_mnist()

        # we use the configuration to control the settings under which we render the stimuli
        receptors = BoxEye(extent=config.extent, kernel_size=config.kernel_size)

        # for memory-friendly rendering we can loop over individual sequences
        # and subsets of the dataset
        rendered_sequences = []
        subset_idx = getattr(config, &quot;subset_idx&quot;, []) or list(range(sequences.shape[0]))
        with tqdm(total=len(subset_idx)) as pbar:
            for index in subset_idx:
                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())
                pbar.update()

        # to join individual sequences along their first dimension
        # to obtain (n_sequences, n_frames, 1, receptors.hexals)
        rendered_sequences = np.concatenate(rendered_sequences, axis=0)

        # the __setattr__ method of the Directory class saves sequences to self.path/&quot;sequences.h5&quot;
        # that can be later retrieved using self.sequences[:]
        self.sequences = rendered_sequences
</code></pre>
<pre><code class="language-python">class CustomStimuli(SequenceDataset):
    # implementing the SequenceDataset interface
    dt = 1 / 100
    framerate = 24
    t_pre = 0.5
    t_post = 0.5
    n_sequences = None
    augment = False

    def __init__(self, rendered_data_config: dict):
        self.dir = RenderedData(rendered_data_config)
        self.sequences = torch.tensor(self.dir.sequences[:])
        self.n_sequences = self.sequences.shape[0]

    def get_item(self, key):
        sequence = self.sequences[key]
        # to match the framerate to the integration time dt, we can resample frames
        # from these indices. note, when dt = 1/framerate, this will return the exact sequence
        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])
        return sequence[resample]
</code></pre>
<pre><code class="language-python"># note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument
data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))
</code></pre>
<h5 id="select-a-pretrained-network">Select a pretrained network</h5>
<p>To select a network from the ensemble of 50 pretrained networks, let's see what our options are.</p>
<p>Paths to pretrained models from the ensemble end with four digit numbers which are sorted by task error (0-49 from best to worst).</p>
<pre><code class="language-python">sorted([
    p.relative_to(flyvision.results_dir)
    for p in (flyvision.results_dir / &quot;flow/0000&quot;).iterdir()
    if p.name.isnumeric()
])
</code></pre>
<pre><code>[PosixPath('flow/0000/000'),
 PosixPath('flow/0000/001'),
 PosixPath('flow/0000/002'),
 PosixPath('flow/0000/003'),
 PosixPath('flow/0000/004'),
 PosixPath('flow/0000/005'),
 PosixPath('flow/0000/006'),
 PosixPath('flow/0000/007'),
 PosixPath('flow/0000/008'),
 PosixPath('flow/0000/009'),
 PosixPath('flow/0000/010'),
 PosixPath('flow/0000/011'),
 PosixPath('flow/0000/012'),
 PosixPath('flow/0000/013'),
 PosixPath('flow/0000/014'),
 PosixPath('flow/0000/015'),
 PosixPath('flow/0000/016'),
 PosixPath('flow/0000/017'),
 PosixPath('flow/0000/018'),
 PosixPath('flow/0000/019'),
 PosixPath('flow/0000/020'),
 PosixPath('flow/0000/021'),
 PosixPath('flow/0000/022'),
 PosixPath('flow/0000/023'),
 PosixPath('flow/0000/024'),
 PosixPath('flow/0000/025'),
 PosixPath('flow/0000/026'),
 PosixPath('flow/0000/027'),
 PosixPath('flow/0000/028'),
 PosixPath('flow/0000/029'),
 PosixPath('flow/0000/030'),
 PosixPath('flow/0000/031'),
 PosixPath('flow/0000/032'),
 PosixPath('flow/0000/033'),
 PosixPath('flow/0000/034'),
 PosixPath('flow/0000/035'),
 PosixPath('flow/0000/036'),
 PosixPath('flow/0000/037'),
 PosixPath('flow/0000/038'),
 PosixPath('flow/0000/039'),
 PosixPath('flow/0000/040'),
 PosixPath('flow/0000/041'),
 PosixPath('flow/0000/042'),
 PosixPath('flow/0000/043'),
 PosixPath('flow/0000/044'),
 PosixPath('flow/0000/045'),
 PosixPath('flow/0000/046'),
 PosixPath('flow/0000/047'),
 PosixPath('flow/0000/048'),
 PosixPath('flow/0000/049')]
</code></pre>
<p>We use the <code>NetworkView</code> class to point to a model. This object can implement plots plus methods to initialize network, stimuli etc. </p>
<pre><code class="language-python">network_view = flyvision.network.NetworkView(flyvision.results_dir / &quot;flow/0000/000&quot;)
</code></pre>
<pre><code>[2024-10-04 16:51:36] network:1001 Initialized network view at /groups/turaga/home/lappalainenj/FlyVis/private/flyvision/data/results/flow/0000/000.
</code></pre>
<pre><code class="language-python"># to load the Pytorch module with pretrained parameters
network = network_view.init_network()
</code></pre>
<pre><code>[2024-10-04 16:51:46] network:253 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
[2024-10-04 16:51:46] chkpt_utils:72 Recovered network state.
</code></pre>
<pre><code class="language-python">movie_input = data[0]
</code></pre>
<pre><code class="language-python">movie_input.shape
</code></pre>
<pre><code>torch.Size([84, 1, 721])
</code></pre>
<h5 id="compute-a-stationary-state">Compute a stationary state</h5>
<p>We initialize the network at a stationary state, to remove transient responses due to stimulus onset from functional stimulus responses like motion detection. The network provides two methods for stationary state computation <code>network.fade_in_state</code> and <code>network.steady_state</code>. We use <code>fade_in_state</code> here, which slowly ramps up
the intensity of the first frame in the sequence to compute a stationary state that minimizes the transient response. The method <code>steady_state</code> computes a sequence-independent stationary state by providing a whole-field grey-scale stimulus at medium intensity (but it does not get rid of a transient response).</p>
<pre><code class="language-python">stationary_state = network.fade_in_state(1.0, data.dt, movie_input[[0]])
</code></pre>
<h5 id="obtain-network-responses">Obtain network responses</h5>
<p>A convenient way to obtain network responses is to call <code>network.simulate</code>
which calls the forward function of the Pytorch module without tracking gradients
(plus it provides a simpler interface than <code>network.forward</code> because it already maps stimulus to receptors using the <code>network.stimulus</code> attribute).</p>
<pre><code class="language-python"># For analysis, we move the returned tensor to cpu.
responses = network.simulate(
    movie_input[None], data.dt, initial_state=stationary_state
).cpu()
</code></pre>
<pre><code class="language-python">responses.shape
</code></pre>
<pre><code>torch.Size([1, 84, 45669])
</code></pre>
<h5 id="visualize-responses-of-specific-cells">Visualize responses of specific cells</h5>
<p><code>LayerActivity</code> is an interface to the response tensor of 45k cells that allows dict- and attribute-style access to the responses of individual cell types and to the responses of their central cells.</p>
<pre><code class="language-python">responses = LayerActivity(responses, network.connectome, keepref=True)
</code></pre>
<pre><code class="language-python">cell_type = &quot;T4c&quot;
</code></pre>
<p>The stimulus on the left, and the response on the right described by passive point neuron voltage dynamics. Cells depolarize (red) and hyperpolarize (blue) in response to the stimulus. A single "hexal" corresponds to one neuron of the cell type.</p>
<pre><code class="language-python">anim = animations.StimulusResponse(movie_input[None], responses[cell_type][:, :, None])
anim.animate_in_notebook(frames=np.arange(anim.frames)[::2])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_66_0.png" /></p>
<p>Often, we are interested in a canonical response of a specific cell type to a specific stimulus to generate hypotheses for their role in a computation. In our model, we can take the central cell as a proxy for all cells of the given type, because cells share their parameters and in- and output connections. I.e. the responses of all cells of a given type would be the same (not taking boundary effects into account) when the same stimulus would cross their identical but spatially offset receptive field in the same way.</p>
<pre><code class="language-python">n_frames = movie_input.shape[0]
time = np.arange(0, n_frames * data.dt, data.dt)
</code></pre>
<pre><code class="language-python">fig, ax = plt_utils.init_plot([2, 2], fontsize=5)
ax.plot(time, responses.central[cell_type].squeeze())
ax.set_xlabel(&quot;time in s&quot;, fontsize=5)
ax.set_ylabel(&quot;central response (a.u.)&quot;, fontsize=5)
</code></pre>
<pre><code>Text(0, 0.5, 'central response (a.u.)')
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_69_1.png" /></p>
<h2 id="compute-responses-over-the-whole-ensemble">Compute responses over the whole ensemble</h2>
<p>In addition to looking at individual models, we next compute responses across the whole ensemble at once to look at them jointly.</p>
<pre><code class="language-python">from typing import List
from tqdm import tqdm
import torch
import numpy as np

np.random.seed(42)
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 200

from pathlib import Path
from datamate import root, Directory

import flyvision
from flyvision.utils.dataset_utils import load_moving_mnist
from flyvision.datasets.datasets import SequenceDataset
from flyvision.utils.activity_utils import LayerActivity
from flyvision import EnsembleView
</code></pre>
<pre><code class="language-python"># the Directory class is a smart pointer to a specific directory
# on the filesystem

# directory to store the rendered stimuli
from flyvision import renderings_dir


# root tells where the Directory-tree starts
@root(renderings_dir)
class RenderedData(Directory):
    class Config(dict):
        extent: int  # radius, in number of receptors of the hexagonal array.
        kernel_size: int  # photon collection radius, in pixels.
        subset_idx: List[int]  # if specified, subset of sequences to render

    def __init__(self, config: Config):
        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.
        # this code will be executed automatically once for each unique configuration to store preprocessed
        # data on disk and later simply provide a reference to it.
        sequences = load_moving_mnist()

        # we use the configuration to control the settings under which we render the stimuli
        receptors = BoxEye(extent=config.extent, kernel_size=config.kernel_size)

        # for memory-friendly rendering we can loop over individual sequences
        # and subsets of the dataset
        rendered_sequences = []
        subset_idx = getattr(config, &quot;subset_idx&quot;, []) or list(range(sequences.shape[0]))
        with tqdm(total=len(subset_idx)) as pbar:
            for index in subset_idx:
                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())
                pbar.update()

        # to join individual sequences along their first dimension
        # to obtain (n_sequences, n_frames, 1, receptors.hexals)
        rendered_sequences = np.concatenate(rendered_sequences, axis=0)

        # the __setattr__ method of the Directory class saves sequences to self.path/&quot;sequences.h5&quot;
        # that can be later retrieved using self.sequences[:]
        self.sequences = rendered_sequences
</code></pre>
<pre><code class="language-python">class CustomStimuli(SequenceDataset):
    # implementing the SequenceDataset interface
    dt = 1 / 100
    framerate = 24
    t_pre = 0.5
    t_post = 0.5
    n_sequences = None
    augment = False

    def __init__(self, rendered_data_config: dict):
        self.dir = RenderedData(rendered_data_config)
        self.sequences = torch.tensor(self.dir.sequences[:])
        self.n_sequences = self.sequences.shape[0]

    def get_item(self, key):
        sequence = self.sequences[key]
        # to match the framerate to the integration time dt, we can resample frames
        # from these indices. note, when dt = 1/framerate, this will return the exact sequence
        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])
        return sequence[resample]
</code></pre>
<pre><code class="language-python"># note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument
data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))
</code></pre>
<h5 id="select-the-pretrained-ensemble">Select the pretrained ensemble</h5>
<p>Similar to the <code>NetworkView</code> object, the <code>EnsembleView</code> object points to an ensemble and implements plots plus methods to initialize networks, stimuli etc. This object provides dict- and attribute-style access to individual <code>NetworkView</code> instances.</p>
<pre><code class="language-python">ensemble = EnsembleView(flyvision.results_dir / &quot;flow/0000&quot;)
</code></pre>
<pre><code>Loading ensemble:   0%|          | 0/50 [00:00&lt;?, ?it/s]


[2024-10-04 16:52:11] ensemble:141 Loaded 50 networks.
</code></pre>
<h5 id="simulate-responses-for-each-network">Simulate responses for each network</h5>
<pre><code class="language-python">movie_input = data[0]
</code></pre>
<p><code>ensemble.simulate</code> provides an efficient method to return responses of all networks within the ensemble.</p>
<pre><code class="language-python"># ensemble.simulate returns an iterator over `network.simulate` for each network.
# we exhaust it and stack responses from all models in the first dimension
responses = np.array(list(ensemble.simulate(movie_input[None], data.dt, fade_in=True)))
</code></pre>
<pre><code>Simulating network:   0%|          | 0/50 [00:00&lt;?, ?it/s]


[2024-10-04 16:52:20] network:253 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.
[2024-10-04 16:52:20] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:20] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:21] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:21] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:21] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:21] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:21] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:22] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:23] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:23] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:23] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:23] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:23] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:24] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:24] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:24] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:24] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:24] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:25] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:25] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:25] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:25] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:25] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:26] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:26] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:26] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:26] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:26] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:27] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:27] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:27] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:27] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:27] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:28] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:28] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:28] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:28] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:28] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:29] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:29] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:29] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:29] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:29] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:30] chkpt_utils:72 Recovered network state.
[2024-10-04 16:52:30] chkpt_utils:72 Recovered network state.
</code></pre>
<pre><code class="language-python"># dims are (n_models, n_sequences, n_frames, n_cells)
responses.shape
</code></pre>
<pre><code>(50, 1, 84, 45669)
</code></pre>
<h5 id="visualize-responses-of-specific-cells-across-the-ensemble">Visualize responses of specific cells across the ensemble</h5>
<pre><code class="language-python">responses = LayerActivity(responses, ensemble[0].connectome, keepref=True)
</code></pre>
<p>We look at responses of all cells of a specific cell-type in the hexagonal lattice.</p>
<pre><code class="language-python">cell_type = &quot;T4c&quot;

# (n_models, n_sequences, n_frames, n_hexals)
responses[cell_type].shape
</code></pre>
<pre><code>(50, 1, 84, 721)
</code></pre>
<p>We can look at all model responses in succession to see how the stimulus causes depolarization and hyperpolarization in the cells. To speed this up a bit, we specify <code>frames</code> to look at every tenth frame.</p>
<pre><code class="language-python">anim = animations.StimulusResponse(movie_input[None], responses[cell_type][:, 0, :, None])
# these are now just the first 5 models for illustration
model_index = [0, 1, 2, 3, 4]
anim.animate_in_notebook(samples=model_index, frames=np.arange(anim.frames)[::10])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_88_0.png" /></p>
<p>Or look at responses in multiple models jointly.
Disclaimer: including more axes slows down the animation.</p>
<pre><code class="language-python">cell_type_responses = responses[cell_type]
model_idx = [0, 1, 2, 3, 4]
anim = animations.StimulusResponse(
    movie_input[None], [cell_type_responses[i][None, 0, :, None] for i in model_idx]
)
anim.animate_in_notebook(frames=np.arange(anim.frames)[::10])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_90_0.png" /></p>
<p>Let's look at how the whole ensemble characterizes the central cell responses.</p>
<pre><code class="language-python">central_responses = responses.central
</code></pre>
<pre><code class="language-python">n_frames = movie_input.shape[0]
time = np.arange(0, n_frames * data.dt, data.dt)
</code></pre>
<pre><code class="language-python">colors = ensemble.task_error().colors
</code></pre>
<pre><code class="language-python">fig, ax = plt_utils.init_plot([2, 2], fontsize=5)
for model_id, response in enumerate(central_responses[cell_type]):
    ax.plot(time, response.squeeze(), c=colors[model_id], zorder=len(ensemble) - model_id)
ax.set_xlabel(&quot;time in s&quot;, fontsize=5)
ax.set_ylabel(&quot;response (a.u.)&quot;, fontsize=5)
ax.set_title(f&quot;{cell_type} responses across the ensemble&quot;, fontsize=5)
</code></pre>
<pre><code>Text(0.5, 1.0, 'T4c responses across the ensemble')
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_95_1.png" /></p>
<p>From the above plot it seems like different models generate different predictions for the cell type function and its hard to tell them apart. Therefore, we clustered the models such that we can separate the above responses by functional cluster for a specific cell type. Note, clusters are stored as dictionaries in which the key is the cluster identity and their values are the indices to the corresponding models.</p>
<pre><code class="language-python">cluster_indices = ensemble.cluster_indices(cell_type)
</code></pre>
<pre><code>[2024-10-04 16:52:49] clustering:643 Loaded T4c embedding and clustering from /groups/turaga/home/lappalainenj/FlyVis/private/flyvision/data/results/flow/0000/umap_and_clustering.
</code></pre>
<pre><code class="language-python">for cluster_id, model_idx in cluster_indices.items():
    fig, ax = plt_utils.init_plot([2, 2], fontsize=5)
    for model_id in model_idx:
        response = responses.central[cell_type][model_id]
        ax.plot(
            time, response.squeeze(), c=colors[model_id], zorder=len(ensemble) - model_id
        )
    ax.set_xlabel(&quot;time in s&quot;, fontsize=5)
    ax.set_ylabel(&quot;response (a.u.)&quot;, fontsize=5)
    ax.set_title(f&quot;{cell_type} responses across cluster {cluster_id + 1}&quot;, fontsize=5)
    plt.show()
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_98_0.png" /></p>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_98_1.png" /></p>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_98_2.png" /></p>
<p>For T4c, we know that the first set of models is upwards tuning and the second is downwards tuning (Fig.4c) -- lets try to observe differences in their responses.</p>
<p>We choose the best upwards tuning model and the best downwards tuning model to compare.</p>
<p>We can notice that the spatial location of hyperpolarization and depolarization is switched vertically between both models. </p>
<pre><code class="language-python">cell_type = &quot;T4c&quot;
</code></pre>
<pre><code class="language-python">cluster_indices = ensemble.cluster_indices(cell_type)
</code></pre>
<pre><code class="language-python">anim = animations.StimulusResponse(
    movie_input[None],
    [
        responses[cell_type][[cluster_indices[0][0]], 0][:, :, None],
        responses[cell_type][[cluster_indices[1][0]], 0][:, :, None],
    ],
)
anim.animate_in_notebook(frames=np.arange(anim.frames)[::5])
</code></pre>
<p><img alt="png" src="../07_flyvision_providing_custom_stimuli_files/07_flyvision_providing_custom_stimuli_102_0.png" /></p>













              </article>
            </div>


<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>

      </main>

        <footer class="md-footer">

  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">


    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>

</div>

    </div>
  </div>
</footer>

    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>


    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>


      <script src="../../assets/javascripts/bundle.d6f25eb3.min.js"></script>

        <script src="../../javascripts/switch-logo.js"></script>


  </body>
</html>
