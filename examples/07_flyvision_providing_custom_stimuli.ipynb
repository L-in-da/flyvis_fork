{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb60e7d",
   "metadata": {},
   "source": [
    "# Providing custom stimuli\n",
    "\n",
    "Follow this notebook to learn how to use our models for generating hypothesis about neural computations with custom stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5384d",
   "metadata": {
    "id": "SXV7OZMrugGF"
   },
   "source": [
    "**Select GPU runtime**\n",
    "\n",
    "To run the notebook on a GPU select Menu -> Runtime -> Change runtime type -> GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323b04c",
   "metadata": {
    "cellView": "form",
    "id": "sWG39Aevugmr"
   },
   "outputs": [],
   "source": [
    "# @markdown **Check access to GPU**\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import torch\n",
    "\n",
    "    try:\n",
    "        cuda_name = torch.cuda.get_device_name()\n",
    "        print(f\"Name of the assigned GPU / CUDA device: {cuda_name}\")\n",
    "    except RuntimeError:\n",
    "        import warnings\n",
    "\n",
    "        warnings.warn(\n",
    "            \"You have not selected Runtime Type: 'GPU' or Google could not assign you one. Please revisit the settings as described above or proceed on CPU (slow).\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db055330",
   "metadata": {
    "id": "-B9TA8nfzmN3"
   },
   "source": [
    "**Install Flyvis**\n",
    "\n",
    "The notebook requires installing our package `flyvis`. You may need to restart your session after running the code block below with Menu -> Runtime -> Restart session. Then, imports from `flyvis` should succeed without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb0ee9",
   "metadata": {
    "cellView": "form",
    "id": "Dkhfe5XBuksW"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    #@markdown **Install Flyvis**\n",
    "    %%capture\n",
    "    !git clone https://github.com/flyvis/flyvis-dev.git\n",
    "    %cd /content/flyvis-dev\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785107ea",
   "metadata": {},
   "source": [
    "## Example dataset\n",
    "\n",
    "We take the public [Moving MNIST](https://www.cs.toronto.edu/~nitish/unsupervised_video/) sequence dataset as an example for a custom stimulus dataset.\n",
    "Moving MNIST consists of short grey-scale videos of numbers from 1-10 which move in arbitrary directions. The dataset entails 10,000 sequences of 20 frames each. Individual frames are 64x64 pixels in height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = load_moving_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole dataset has dims (n_sequences, n_frames, height, width)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da99dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = flyvision.animations.Imshow(sequences, cmap=plt.cm.binary_r)\n",
    "animation.animate_in_notebook(samples=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af135568",
   "metadata": {},
   "source": [
    "Alternative: for an alternative dataset that is generated at runtime and does not require a download try `random_walk_of_blocks`. As a simple drop-in replacement, this requires to replace `load_moving_mnist` with `random_walk_of_blocks` across the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flyvision.utils.dataset_utils import random_walk_of_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = random_walk_of_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = flyvision.animations.Imshow(sequences, cmap=plt.cm.binary_r)\n",
    "animation.animate_in_notebook(samples=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fd4b5",
   "metadata": {},
   "source": [
    "## BoxEye rendering\n",
    "\n",
    "##### Rendering cartesian images to hexagonal lattice\n",
    "\n",
    "We translate cartesian frames into receptor activations by placing simulated photoreceptors in a two-dimensional hexagonal array in pixel space (blue dots below), 31 columns across resulting in 721 columns in total, spaced 13 pixels apart. The transduced luminance at each photoreceptor is the greyscale mean value in the 13Ã—13-pixel region surrounding it (black boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f73f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flyvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptors = flyvision.rendering.BoxEye(extent=15, kernel_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a2223",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = receptors.illustrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968ca3c",
   "metadata": {},
   "source": [
    "### Render a single frame\n",
    "\n",
    "To illustrate, this is what rendering a single frame looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a752cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = load_moving_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_frame = sequences[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = flyvision.plots.plt_utils.init_plot(figsize=[1, 1], fontsize=5)\n",
    "ax = flyvision.plots.plt_utils.rm_spines(ax)\n",
    "ax.imshow(single_frame, cmap=plt.cm.binary_r)\n",
    "_ = ax.set_title('example frame', fontsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7518bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rendering uses pytorch native Conv2d module so it can be executed on GPU and fast\n",
    "# we first move the frame to GPU\n",
    "single_frame = torch.tensor(single_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the inputs to the receptors instance must have four dimensions (samples, frames, height, width),\n",
    "# we create two empty dimensions for samples and frames\n",
    "single_frame = single_frame[None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to render the single frame we simply call the instance\n",
    "# this automatically rescales the frame to match the receptor layout as illustrated above\n",
    "# and then places the average pixel value of the 13x13 boxes at the receptor positions\n",
    "receptors = flyvision.rendering.BoxEye()\n",
    "rendered = receptors(single_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13807af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 721 receptor coordinates are implicitly given in the last dimension\n",
    "# they correspond to sorted hexagonal coordinates (u-coordinate, v-coordinate, value)\n",
    "rendered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7a236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the rendered frame is a slightly blurred version of the example\n",
    "fig, ax, _ = flyvision.plots.plots.quick_hex_scatter(\n",
    "    rendered.squeeze(), vmin=0, vmax=1, cbar_x_offset=0, fontsize=5\n",
    ")\n",
    "_ = ax.set_title(\"example frame rendered\", fontsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71431e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disclaimer: thinking in hex coordinates can be unfamiliar.\n",
    "# Therefore, we circumvent dealing with them explicitly.\n",
    "# Still - to understand how the above plot infers the pixel-plane coordinates \n",
    "# from the implicit hexagonal coordinates, you can inspect the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we can explicitly create sorted hex-coordinates from the integer radius of the hexagonal grid\n",
    "# # for a regular hexagonal lattice, the radius is uniquely determined from the number of hexagons\n",
    "\n",
    "# radius = flyvision.utils.hex_utils.get_hextent(rendered.shape[-1])\n",
    "\n",
    "# # here we create integer u, v coordinates, and we stick to the same function and convention \n",
    "# # everywhere in the code\n",
    "# u, v = flyvision.utils.hex_utils.get_hex_coords(radius)\n",
    "\n",
    "# # we transform them to pixel coordinates using our convention\n",
    "# x, y = flyvision.utils.hex_utils.hex_to_pixel(u, v)\n",
    "\n",
    "# # and can just scatter them to be back at the photoreceptor layout\n",
    "# fig, ax = flyvision.plots.plt_utils.init_plot(figsize=[2, 2], fontsize=5)\n",
    "# ax.scatter(x, y, s=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f3e02",
   "metadata": {},
   "source": [
    "## Render a whole dataset to disk\n",
    "\n",
    "We save rendered sequences to disk to retrieve them faster at runtime.\n",
    "\n",
    "We will use our library datamate here because it provides a powerful interface for writing and reading arrayfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c28d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from pathlib import Path\n",
    "from datamate import root, Directory\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72397f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Directory class is a smart pointer to a specific directory\n",
    "# on the filesystem\n",
    "\n",
    "# directory to store the rendered stimuli\n",
    "from flyvision import renderings_dir\n",
    "\n",
    "\n",
    "# root tells where the Directory-tree starts\n",
    "@root(renderings_dir)\n",
    "class RenderedData(Directory):\n",
    "    class Config(dict):\n",
    "        extent: int  # radius, in number of receptors of the hexagonal array.\n",
    "        kernel_size: int  # photon collection radius, in pixels.\n",
    "        subset_idx: List[int]  # if specified, subset of sequences to render\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.\n",
    "        # this code will be executed automatically once for each unique configuration to store preprocessed\n",
    "        # data on disk and later simply provide a reference to it.\n",
    "        sequences = load_moving_mnist()\n",
    "\n",
    "        # we use the configuration to control the settings under which we render the stimuli\n",
    "        receptors = flyvision.rendering.BoxEye(\n",
    "            extent=config.extent, kernel_size=config.kernel_size\n",
    "        )\n",
    "\n",
    "        # for memory-friendly rendering we can loop over individual sequences\n",
    "        # and subsets of the dataset\n",
    "        rendered_sequences = []\n",
    "        subset_idx = getattr(config, \"subset_idx\", []) or list(\n",
    "            range(sequences.shape[0])\n",
    "        )\n",
    "        with tqdm(total=len(subset_idx)) as pbar:\n",
    "            for index in subset_idx:\n",
    "                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())\n",
    "                pbar.update()\n",
    "\n",
    "        # to join individual sequences along their first dimension\n",
    "        # to obtain (n_sequences, n_frames, 1, receptors.hexals)\n",
    "        rendered_sequences = np.concatenate(rendered_sequences, axis=0)\n",
    "\n",
    "        # the __setattr__ method of the Directory class saves sequences to self.path/\"sequences.h5\"\n",
    "        # that can be later retrieved using self.sequences[:]\n",
    "        self.sequences = rendered_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument \n",
    "moving_mnist_rendered = RenderedData(\n",
    "    dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we can retrieve the sequences from the disk into memory\n",
    "rendered_sequences = moving_mnist_rendered.sequences[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88363b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125935c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "animation = flyvision.animations.HexScatter(rendered_sequences, vmin=0, vmax=1)\n",
    "animation.animate_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, to delete a Directory, e.g. to change the __init__ and reinstantiate,\n",
    "# run moving_mnist_rendered.rmtree(\"y\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cf57d",
   "metadata": {},
   "source": [
    "## Create a sequence dataset\n",
    "\n",
    "Next we create a Pytorch dataset for loading the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeceb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from pathlib import Path\n",
    "from datamate import root, Directory\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist\n",
    "from flyvision.datasets.datasets import SequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Directory class is a smart pointer to a specific directory\n",
    "# on the filesystem\n",
    "\n",
    "# directory to store the rendered stimuli\n",
    "from flyvision import renderings_dir\n",
    "\n",
    "\n",
    "# root tells where the Directory-tree starts\n",
    "@root(renderings_dir)\n",
    "class RenderedData(Directory):\n",
    "    class Config(dict):\n",
    "        extent: int  # radius, in number of receptors of the hexagonal array.\n",
    "        kernel_size: int  # photon collection radius, in pixels.\n",
    "        subset_idx: List[int]  # if specified, subset of sequences to render\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.\n",
    "        # this code will be executed automatically once for each unique configuration to store preprocessed\n",
    "        # data on disk and later simply provide a reference to it.\n",
    "        sequences = load_moving_mnist()\n",
    "\n",
    "        # we use the configuration to control the settings under which we render the stimuli\n",
    "        receptors = flyvision.rendering.BoxEye(\n",
    "            extent=config.extent, kernel_size=config.kernel_size\n",
    "        )\n",
    "\n",
    "        # for memory-friendly rendering we can loop over individual sequences\n",
    "        # and subsets of the dataset\n",
    "        rendered_sequences = []\n",
    "        subset_idx = getattr(config, \"subset_idx\", []) or list(\n",
    "            range(sequences.shape[0])\n",
    "        )\n",
    "        with tqdm(total=len(subset_idx)) as pbar:\n",
    "            for index in subset_idx:\n",
    "                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())\n",
    "                pbar.update()\n",
    "\n",
    "        # to join individual sequences along their first dimension\n",
    "        # to obtain (n_sequences, n_frames, 1, receptors.hexals)\n",
    "        rendered_sequences = np.concatenate(rendered_sequences, axis=0)\n",
    "\n",
    "        # the __setattr__ method of the Directory class saves sequences to self.path/\"sequences.h5\"\n",
    "        # that can be later retrieved using self.sequences[:]\n",
    "        self.sequences = rendered_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07830a51",
   "metadata": {},
   "source": [
    "##### Create a custom dataset\n",
    "We create a generic interface for custom datasets to make dataloading consistent---this interface can tell the sampler what the framerate, the integration time steps, durations for pre-, and post grey-scale stimulation, and the number of sequences are.\n",
    "\n",
    "In this case, we inherit a SequenceDataset, that also obeys (and extends) the interface of Pytorch's Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255310a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStimuli(SequenceDataset):\n",
    "    \n",
    "    # implementing the SequenceDataset interface \n",
    "    dt = 1/100\n",
    "    framerate = 24\n",
    "    t_pre = 0.5\n",
    "    t_post = 0.5\n",
    "    n_sequences = None\n",
    "    augment = False\n",
    "    \n",
    "    def __init__(self, rendered_data_config: dict):\n",
    "        self.dir = RenderedData(rendered_data_config)\n",
    "        self.sequences = torch.tensor(self.dir.sequences[:])\n",
    "        self.n_sequences = self.sequences.shape[0]\n",
    "\n",
    "    def get_item(self, key):\n",
    "        sequence = self.sequences[key]\n",
    "        # to match the framerate to the integration time dt, we can resample frames\n",
    "        # from these indices. note, when dt = 1/framerate, this will return the exact sequence\n",
    "        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])\n",
    "        return sequence[resample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d91b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument \n",
    "data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a89868",
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = flyvision.animations.HexScatter(data[0][None], vmin=0, vmax=1)\n",
    "animation.animate_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80e6d1",
   "metadata": {},
   "source": [
    "## Compute model responses to custom stimuli\n",
    "\n",
    "Now, we can compute model responses across individual models or the whole ensemble to our custom stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from pathlib import Path\n",
    "from datamate import root, Directory\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist\n",
    "from flyvision.datasets.datasets import SequenceDataset\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from flyvision.animations import StimulusResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51203d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Directory class is a smart pointer to a specific directory\n",
    "# on the filesystem\n",
    "\n",
    "# directory to store the rendered stimuli\n",
    "from flyvision import renderings_dir\n",
    "\n",
    "\n",
    "# root tells where the Directory-tree starts\n",
    "@root(renderings_dir)\n",
    "class RenderedData(Directory):\n",
    "    class Config(dict):\n",
    "        extent: int  # radius, in number of receptors of the hexagonal array.\n",
    "        kernel_size: int  # photon collection radius, in pixels.\n",
    "        subset_idx: List[int]  # if specified, subset of sequences to render\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.\n",
    "        # this code will be executed automatically once for each unique configuration to store preprocessed\n",
    "        # data on disk and later simply provide a reference to it.\n",
    "        sequences = load_moving_mnist()\n",
    "\n",
    "        # we use the configuration to control the settings under which we render the stimuli\n",
    "        receptors = flyvision.rendering.BoxEye(\n",
    "            extent=config.extent, kernel_size=config.kernel_size\n",
    "        )\n",
    "\n",
    "        # for memory-friendly rendering we can loop over individual sequences\n",
    "        # and subsets of the dataset\n",
    "        rendered_sequences = []\n",
    "        subset_idx = getattr(config, \"subset_idx\", []) or list(\n",
    "            range(sequences.shape[0])\n",
    "        )\n",
    "        with tqdm(total=len(subset_idx)) as pbar:\n",
    "            for index in subset_idx:\n",
    "                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())\n",
    "                pbar.update()\n",
    "\n",
    "        # to join individual sequences along their first dimension\n",
    "        # to obtain (n_sequences, n_frames, 1, receptors.hexals)\n",
    "        rendered_sequences = np.concatenate(rendered_sequences, axis=0)\n",
    "\n",
    "        # the __setattr__ method of the Directory class saves sequences to self.path/\"sequences.h5\"\n",
    "        # that can be later retrieved using self.sequences[:]\n",
    "        self.sequences = rendered_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStimuli(SequenceDataset):\n",
    "    \n",
    "    # implementing the SequenceDataset interface \n",
    "    dt = 1/100\n",
    "    framerate = 24\n",
    "    t_pre = 0.5\n",
    "    t_post = 0.5\n",
    "    n_sequences = None\n",
    "    augment = False\n",
    "    \n",
    "    def __init__(self, rendered_data_config: dict):\n",
    "        self.dir = RenderedData(rendered_data_config)\n",
    "        self.sequences = torch.tensor(self.dir.sequences[:])\n",
    "        self.n_sequences = self.sequences.shape[0]\n",
    "\n",
    "    def get_item(self, key):\n",
    "        sequence = self.sequences[key]\n",
    "        # to match the framerate to the integration time dt, we can resample frames\n",
    "        # from these indices. note, when dt = 1/framerate, this will return the exact sequence\n",
    "        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])\n",
    "        return sequence[resample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b69c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument \n",
    "data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4bfa0",
   "metadata": {},
   "source": [
    "##### Select a pretrained network\n",
    "\n",
    "To select a network from the ensemble of 50 pretrained networks, let's see what our options are.\n",
    "\n",
    "Paths to pretrained models from the ensemble end with four digit numbers which are sorted by task error (0-49 from best to worst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(\n",
    "    [\n",
    "        p.relative_to(flyvision.results_dir)\n",
    "        for p in (flyvision.results_dir / \"flow/0000\").iterdir()\n",
    "        if p.name.isnumeric()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45367e",
   "metadata": {},
   "source": [
    "We use the `NetworkView` class to point to a model. This object can implement plots plus methods to initialize network, stimuli etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_view = flyvision.network.NetworkView(flyvision.results_dir / \"flow/0000/000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the Pytorch module with pretrained parameters\n",
    "network = network_view.init_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62865670",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_input = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43857246",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58269821",
   "metadata": {},
   "source": [
    "##### Compute a stationary state\n",
    "\n",
    "We initialize the network at a stationary state, to remove transient responses due to stimulus onset from functional stimulus responses like motion detection. The network provides two methods for stationary state computation `network.fade_in_state` and `network.steady_state`. We use `fade_in_state` here, which slowly ramps up\n",
    "the intensity of the first frame in the sequence to compute a stationary state that minimizes the transient response. The method `steady_state` computes a sequence-independent stationary state by providing a whole-field grey-scale stimulus at medium intensity (but it does not get rid of a transient response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a632d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_state = network.fade_in_state(1.0, data.dt, movie_input[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2d6b5",
   "metadata": {},
   "source": [
    "##### Obtain network responses\n",
    "\n",
    "A convenient way to obtain network responses is to call `network.simulate`\n",
    "which calls the forward function of the Pytorch module without tracking gradients\n",
    "(plus it provides a simpler interface than `network.forward` because it already maps stimulus to receptors using the `network.stimulus` attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis, we move the returned tensor to cpu.\n",
    "responses = network.simulate(movie_input[None], data.dt, initial_state=stationary_state).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bf7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a76d97",
   "metadata": {},
   "source": [
    "##### Visualize responses of specific cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a4b3c",
   "metadata": {},
   "source": [
    "`LayerActivity` is an interface to the response tensor of 45k cells that allows dict- and attribute-style access to the responses of individual cell types and to the responses of their central cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a148c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = LayerActivity(responses, network.connectome, keepref=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a66a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = \"T4c\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64eb4f7",
   "metadata": {},
   "source": [
    "The stimulus on the left, and the response on the right described by passive point neuron voltage dynamics. Cells depolarize (red) and hyperpolarize (blue) in response to the stimulus. A single \"hexal\" corresponds to one neuron of the cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = StimulusResponse(\n",
    "    movie_input[None],\n",
    "    responses[cell_type][:, :, None]\n",
    ")\n",
    "anim.animate_in_notebook(frames=np.arange(anim.frames)[::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac1f9c",
   "metadata": {},
   "source": [
    "Often, we are interested in a canonical response of a specific cell type to a specific stimulus to generate hypotheses for their role in a computation. In our model, we can take the central cell as a proxy for all cells of the given type, because cells share their parameters and in- and output connections. I.e. the responses of all cells of a given type would be the same (not taking boundary effects into account) when the same stimulus would cross their identical but spatially offset receptive field in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51883677",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = movie_input.shape[0]\n",
    "time = np.arange(0, n_frames * data.dt, data.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95409433",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = flyvision.plots.plt_utils.init_plot([2, 2], fontsize=5)\n",
    "ax.plot(time, responses.central[cell_type].squeeze())\n",
    "ax.set_xlabel(\"time in s\", fontsize=5)\n",
    "ax.set_ylabel(\"central response (a.u.)\", fontsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d4a6c",
   "metadata": {},
   "source": [
    "## Compute responses over the whole ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8121ee",
   "metadata": {},
   "source": [
    "In addition to looking at individual models, we next compute responses across the whole ensemble at once to look at them jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637389f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "from pathlib import Path\n",
    "from datamate import root, Directory\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.dataset_utils import load_moving_mnist\n",
    "from flyvision.datasets.datasets import SequenceDataset\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from flyvision.animations import StimulusResponse\n",
    "from flyvision.ensemble import EnsembleView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Directory class is a smart pointer to a specific directory\n",
    "# on the filesystem\n",
    "\n",
    "# directory to store the rendered stimuli\n",
    "from flyvision import renderings_dir\n",
    "\n",
    "\n",
    "# root tells where the Directory-tree starts\n",
    "@root(renderings_dir)\n",
    "class RenderedData(Directory):\n",
    "    class Config(dict):\n",
    "        extent: int  # radius, in number of receptors of the hexagonal array.\n",
    "        kernel_size: int  # photon collection radius, in pixels.\n",
    "        subset_idx: List[int]  # if specified, subset of sequences to render\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        # here comes the preprocessing and rendering as above or similar -- depending on the dataset etc.\n",
    "        # this code will be executed automatically once for each unique configuration to store preprocessed\n",
    "        # data on disk and later simply provide a reference to it.\n",
    "        sequences = load_moving_mnist()\n",
    "\n",
    "        # we use the configuration to control the settings under which we render the stimuli\n",
    "        receptors = flyvision.rendering.BoxEye(\n",
    "            extent=config.extent, kernel_size=config.kernel_size\n",
    "        )\n",
    "\n",
    "        # for memory-friendly rendering we can loop over individual sequences\n",
    "        # and subsets of the dataset\n",
    "        rendered_sequences = []\n",
    "        subset_idx = getattr(config, \"subset_idx\", []) or list(\n",
    "            range(sequences.shape[0])\n",
    "        )\n",
    "        with tqdm(total=len(subset_idx)) as pbar:\n",
    "            for index in subset_idx:\n",
    "                rendered_sequences.append(receptors(sequences[[index]]).cpu().numpy())\n",
    "                pbar.update()\n",
    "\n",
    "        # to join individual sequences along their first dimension\n",
    "        # to obtain (n_sequences, n_frames, 1, receptors.hexals)\n",
    "        rendered_sequences = np.concatenate(rendered_sequences, axis=0)\n",
    "\n",
    "        # the __setattr__ method of the Directory class saves sequences to self.path/\"sequences.h5\"\n",
    "        # that can be later retrieved using self.sequences[:]\n",
    "        self.sequences = rendered_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStimuli(SequenceDataset):\n",
    "    \n",
    "    # implementing the SequenceDataset interface \n",
    "    dt = 1/100\n",
    "    framerate = 24\n",
    "    t_pre = 0.5\n",
    "    t_post = 0.5\n",
    "    n_sequences = None\n",
    "    augment = False\n",
    "    \n",
    "    def __init__(self, rendered_data_config: dict):\n",
    "        self.dir = RenderedData(rendered_data_config)\n",
    "        self.sequences = torch.tensor(self.dir.sequences[:])\n",
    "        self.n_sequences = self.sequences.shape[0]\n",
    "\n",
    "    def get_item(self, key):\n",
    "        sequence = self.sequences[key]\n",
    "        # to match the framerate to the integration time dt, we can resample frames\n",
    "        # from these indices. note, when dt = 1/framerate, this will return the exact sequence\n",
    "        resample = self.get_temporal_sample_indices(sequence.shape[0], sequence.shape[0])\n",
    "        return sequence[resample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, to render the whole dataset provide an empty list for `subset_idx` or delete the key word argument \n",
    "data = CustomStimuli(dict(extent=15, kernel_size=13, subset_idx=[0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8861c",
   "metadata": {},
   "source": [
    "##### Select the pretrained ensemble\n",
    "\n",
    "Similar to the `NetworkView` object, the `EnsembleView` object points to an ensemble and implements plots plus methods to initialize networks, stimuli etc. This object provides dict- and attribute-style access to individual `NetworkView` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleView(flyvision.results_dir / \"flow/0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599702f",
   "metadata": {},
   "source": [
    "##### Simulate responses for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_input = data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02995f",
   "metadata": {},
   "source": [
    "`ensemble.simulate` provides an efficient method to return responses of all networks within the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e34ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble.simulate returns an iterator over `network.simulate` for each network.\n",
    "# we exhaust it and stack responses from all models in the first dimension\n",
    "responses = np.array(list(ensemble.simulate(movie_input[None], data.dt, fade_in=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims are (n_models, n_sequences, n_frames, n_cells)\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87143e0c",
   "metadata": {},
   "source": [
    "##### Visualize responses of specific cells across the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99728ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = LayerActivity(responses, ensemble[0].connectome, keepref=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389679e",
   "metadata": {},
   "source": [
    "We look at responses of all cells of a specific cell-type in the hexagonal lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = \"T4c\"\n",
    "\n",
    "# (n_models, n_sequences, n_frames, n_hexals)\n",
    "responses[cell_type].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1aa3d9",
   "metadata": {},
   "source": [
    "We can look at all model responses in succession to see how the stimulus causes depolarization and hyperpolarization in the cells. To speed this up a bit, we specify `frames` to look at every tenth frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = flyvision.animations.activations.StimulusResponse(\n",
    "    movie_input[None], responses[cell_type][:, 0, :, None]\n",
    ")\n",
    "# these are now just the first 5 models for illustration\n",
    "model_index = [0, 1, 2, 3, 4]\n",
    "anim.animate_in_notebook(\n",
    "    samples=model_index,  \n",
    "    frames=np.arange(anim.frames)[::10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b41244",
   "metadata": {},
   "source": [
    "Or look at responses in multiple models jointly. \n",
    "Disclaimer: including more axes slows down the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_responses = responses[cell_type]\n",
    "model_idx = [0, 1, 2, 3, 4]\n",
    "anim = flyvision.animations.activations.StimulusResponse(\n",
    "    movie_input[None],\n",
    "    [cell_type_responses[i][None, 0, :, None] for i in model_idx]\n",
    ")\n",
    "anim.animate_in_notebook(frames=np.arange(anim.frames)[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aadb19",
   "metadata": {},
   "source": [
    "Let's look at how the whole ensemble characterizes the central cell responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_responses = responses.central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = movie_input.shape[0]\n",
    "time = np.arange(0, n_frames * data.dt, data.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fa8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ensemble.task_error().colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49675e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = flyvision.plots.plt_utils.init_plot([2, 2], fontsize=5)\n",
    "for model_id, response in enumerate(central_responses[cell_type]):\n",
    "    ax.plot(time, response.squeeze(), c=colors[model_id], zorder=len(ensemble) - model_id)\n",
    "ax.set_xlabel(\"time in s\", fontsize=5)\n",
    "ax.set_ylabel(\"response (a.u.)\", fontsize=5)\n",
    "ax.set_title(f\"{cell_type} responses across the ensemble\", fontsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93535ef",
   "metadata": {},
   "source": [
    "From the above plot it seems like different models generate different predictions for the cell type function and its hard to tell them apart. Therefore, we clustered the models such that we can separate the above responses by functional cluster for a specific cell type. Note, clusters are stored as dictionaries in which the key is the cluster identity and their values are the indices to the corresponding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddced97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices = ensemble.cluster_indices(cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938adf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, model_idx in cluster_indices.items():\n",
    "    fig, ax = flyvision.plots.plt_utils.init_plot([2, 2], fontsize=5)\n",
    "    for model_id in model_idx:\n",
    "        response = responses.central[cell_type][model_id]\n",
    "        ax.plot(time, response.squeeze(), c=colors[model_id], zorder=len(ensemble) - model_id)\n",
    "    ax.set_xlabel(\"time in s\", fontsize=5)\n",
    "    ax.set_ylabel(\"response (a.u.)\", fontsize=5)\n",
    "    ax.set_title(f\"{cell_type} responses across cluster {cluster_id+1}\", fontsize=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e797c",
   "metadata": {},
   "source": [
    "For T4c, we know that the first set of models is upwards tuning and the second is downwards tuning (Fig.4c) -- lets try to observe differences in their responses.\n",
    "\n",
    "We choose the best upwards tuning model and the best downwards tuning model to compare.\n",
    "\n",
    "We can notice that the spatial location of hyperpolarization and depolarization is switched vertically between both models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = \"T4c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices = ensemble.cluster_indices(cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ade61",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = flyvision.animations.activations.StimulusResponse(\n",
    "    movie_input[None],\n",
    "    [responses[cell_type][[cluster_indices[0][0]], 0][:, :, None], \n",
    "     responses[cell_type][[cluster_indices[1][0]], 0][:, :, None]]\n",
    ")\n",
    "anim.animate_in_notebook(frames = np.arange(anim.frames)[::5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
